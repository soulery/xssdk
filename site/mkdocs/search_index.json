{
    "docs": [
        {
            "location": "/", 
            "text": "Citrix XenServer 7.0 Software Development Kit\n\n\nChapter 1. Introduction\n\n\nWelcome to the developer's guide for XenServer. Here you will find the information you need in order to understand and use the Software Development Kit (SDK) that XenServer provides. This information will provide you with some of the architectural background and thinking that underpins the APIs, the tools that have been provided, and how to quickly get off the ground.\n\n\nChapter 2. Getting Started\n\n\nXenServer includes a XML-RPC based API providing programmatic access to the extensive set of XenServer management features and tools. The XenServer API can be called from a remote system as well as local to the XenServer host. Although it is possible to write applications which use the XenServer Management API directly through raw XML-RPC calls, the task of developing third-party applications is greatly simplified through the use of a language binding which exposes the individual API calls as first-class functions in the target language. The XenServer SDK provides language bindings and example code for the C, C#, Java, Python and PowerShell programming languages.\n\n\n2.1. System Requirements and Preparation\n\n\nThe first step towards working with the SDK is to install XenServer. A free version, Citrix XenServer, is available for download at http://www.citrix.com/downloads/xenserver/. Please refer to the XenServer Installation Guide for detailed instructions on how to set up your development host. When the installation is complete, please note the host IP address and the host password.\n\n\n2.2. Downloading\n\n\nThe SDK is packaged as a ZIP file and is available as a free download from http://www.citrix.com/downloads/xenserver/ .\n\n\n2.3. Content Map\n\n\nThe extracted contents of the SDK ZIP file are in the XenServer-SDK directory. The following is an overview of its structure. Where necessary, subdirectories have their own individual README files. It should be noted that the examples provided are not the same across all the language bindings, so, if you intend to use one binding, it is advisable to also browse the sample code available in the others.\n- XenServer-SDK. \n\n\n- libxenserver. The XenServer SDK for C.\n\n     - bin. libxenserver binaries.\n\n    - src. libxenserver source code and examples and a Makefile to build them.\n\n- XenServer.NET. The XenServer SDK for C#.NET.\n\n    - bin. XenServer.NET binaries.\n\n    - samples. XenServer.NET examples.\n\n    - src. XenServer.NET source code.\n\n- XenServerJava. The XenServer SDK for Java\n\n    - bin. Java binaries.\n\n    - javadoc. Java documentation.\n\n    - samples. Java examples.\n\n    - src. Java source code and a Makefile to build the code and the examples.\n\n- XenServerPowerShell. The XenServer SDK for PowerShell.\n\n    - XenServerPSModule.  The XenServer PowerShell module.\n\n    - samples. PowerShell example scripts.\n\n    - src. XenServer PowerShell source code.\n\n- XenServerPython. This directory contains the XenServer Python module XenAPI.py. The library provision.py is used by the examples.\n    - samples. XenServer API examples using Python.\n\n\n\n\n\nChapter 3. Overview of the XenServer API\n\n\nThis chapter introduces the XenServer API (subsequently referred to as the \"API\") and its associated object model. The API has the following key features:\n\n\n\n\n\n\nManagement of all aspects of the XenServer Host. The API allows you to manage VMs, storage, networking, host configuration and pools. Performance and status metrics can also be queried from the API.\n\n\n\n\n\n\nPersistent Object Model. The results of all side-effecting operations (e.g. object creation, deletion and parameter modifications) are persisted in a server-side database that is managed by the XenServer installation.\n\n\n\n\n\n\nAn event mechanism. Through the API, clients can register to be notified when persistent (server-side) objects are modified. This enables applications to keep track of datamodel modifications performed by concurrently executing clients.\n\n\n\n\n\n\nSynchronous and asynchronous invocation. All API calls can be invoked synchronously (that is, block until completion); any API call that may be long-running can also be invoked asynchronously. Asynchronous calls return immediately with a reference to a task object. This task object can be queried (through the API) for progress and status information. When an asynchronously invoked operation completes, the result (or error code) is available from the task object.\n\n\n\n\n\n\nRemotable and Cross-Platform. The client issuing the API calls does not have to be resident on the host being managed; nor does it have to be connected to the host over ssh in order to execute the API. API calls make use of the XML-RPC protocol to transmit requests and responses over the network.\n\n\n\n\n\n\nSecure and Authenticated Access. The XML-RPC API server executing on the host accepts secure socket connections. This allows a client to execute the APIs over the https protocol. Further, all the API calls execute in the context of a login session generated through username and password validation at the server. This provides secure and authenticated access to the XenServer installation.\n\n\n\n\n\n\n3.1. Getting Started with the API\n\n\nWe will start our tour of the API by describing the calls required to create a new VM on a XenServer installation, and take it through a start/suspend/resume/stop cycle. This is done without reference to code in any specific language; at this stage we just describe the informal sequence of RPC invocations that accomplish our \"install and start\" task.\n\n\n\n\nNote\n\nWe recommend strongly against using the VM.create call, which may be removed or changed in a future version of the API. Read on to learn other ways to make a new VM.\n\n\n\n\n3.1.1. Authentication: acquiring a session reference\n\n\nThe first step is to call Session.login_with_password(\nusername\n, \npassword\n, \nclient_API_version\n, \noriginator\n). The API is session based, so before you can make other calls you will need to authenticate with the server. Assuming the username and password are authenticated correctly, the result of this call is a session reference. Subsequent API calls take the session reference as a parameter. In this way we ensure that only API users who are suitably authorized can perform operations on a XenServer installation. You can continue to use the same session for any number of API calls. When you have finished the session, Citrix recommends that you call \nSession.logout(session)\n to clean up: see later.\n\n\n3.1.2. Acquiring a list of templates to base a new VM installation on\n\n\nThe next step is to query the list of \"templates\" on the host. Templates are specially-marked VM objects that specify suitable default parameters for a variety of supported guest types. (If you want to see a quick enumeration of the templates on a XenServer installation for yourself then you can execute the xe template-list CLI command.) To get a list of templates from the API, we need to find the VM objects on the server that have their is_a_template field set to true. One way to do this by calling VM.get_all_records(session) where the session parameter is the reference we acquired from our Session.login_with_password call earlier. This call queries the server, returning a snapshot (taken at the time of the call) containing all the VM object references and their field values.\n\n\n(Remember that at this stage we are not concerned about the particular mechanisms by which the returned object references and field values can be manipulated in any particular client language: that detail is dealt with by our language-specific API bindings and described concretely in the following chapter. For now it suffices just to assume the existence of an abstract mechanism for reading and manipulating objects and field values returned by API calls.)\n\n\nNow that we have a snapshot of all the VM objects' field values in the memory of our client application we can simply iterate through them and find the ones that have their \"is_a_template\" set to true. At this stage let's assume that our example application further iterates through the template objects and remembers the reference corresponding to the one that has its \"name_label\" set to \"Debian Etch 4.0\" (one of the default Linux templates supplied with XenServer).\n\n\n3.1.3. Installing the VM based on a template\n\n\nContinuing through our example, we must now install a new VM based on the template we selected. The installation process requires 2 API calls:\n\n\n\n\n\n\nFirst we must now invoke the API call VM.clone(session, t_ref, \"my first VM\"). This tells the server to clone the VM object referenced by t_ref in order to make a new VM object. The return value of this call is the VM reference corresponding to the newly-created VM. Let's call this new_vm_ref.\n\n\n\n\n\n\nAt this stage the object referred to by new_vm_ref is still a template (just like the VM object referred to by t_ref, from which it was cloned). To make new_vm_ref into a VM object we need to call VM.provision(session, new_vm_ref). When this call returns the new_vm_ref object will have had its is_a_template field set to false, indicating that new_vm_ref now refers to a regular VM ready for starting.\n\n\n\n\n\n\n\n\nNote\n\nThe provision operation may take a few minutes, as it is as during this call that the template's disk images are created. In the case of the Debian template, the newly created disks are also at this stage populated with a Debian root filesystem.\n\n\n\n\n3.1.4. Taking the VM through a start/suspend/resume/stop cycle\n\n\nNow we have an object reference representing our newly-installed VM, it is trivial to take it through a few lifecycle operations:\n\n\n\n\n\n\nTo start our VM we can just call VM.start(session, new_vm_ref)\n\n\n\n\n\n\nAfter it's running, we can suspend it by calling VM.suspend(session, new_vm_ref),\n\n\n\n\n\n\nand then resume it by calling VM.resume(session, new_vm_ref).\n\n\n\n\n\n\nWe can call VM.shutdown(session, new_vm_ref) to shutdown the VM cleanly.\n\n\n\n\n\n\n3.1.5. Logging out\n\n\nOnce an application is finished interacting with a XenServer Host it is good practice to call Session.logout(session). This invalidates the session reference (so it cannot be used in subsequent API calls) and simultaneously deallocates server-side memory used to store the session object.\n\n\nAlthough inactive sessions will eventually timeout, the server has a hardcoded limit of 500 concurrent sessions for each username or originator. Once this limit has been reached fresh logins will evict the session objects that have been used least recently, causing their associated session references to become invalid. For successful interoperability with other applications, concurrently accessing the server, the best policy is:\n\n\n\n\n\n\nChoose a string that identifies your application and its version.\n\n\n\n\n\n\nCreate a single session at start-of-day, using that identifying string for the originator parameter to Session.login_with_password.\n\n\n\n\n\n\nUse this session throughout the application (note that sessions can be used across multiple separate client-server network connections) and then explicitly logout when possible.\n\n\n\n\n\n\nIf a poorly written client leaks sessions or otherwise exceeds the limit, then as long as the client uses an appropriate originator argument, it will be easily identifiable from the XenServer logs and XenServer will destroy the longest-idle sessions of the rogue client only; this may cause problems for that client but not for other clients. If the misbehaving client did not specify an originator, it would be harder to identify and would cause the premature destruction of sessions of any clients that also did not specify an originator\n\n\n3.1.6. Install and start example: summary\n\n\nWe have seen how the API can be used to install a VM from a XenServer template and perform a number of lifecycle operations on it. You will note that the number of calls we had to make in order to affect these operations was small:\n\n\n\n\n\n\nOne call to acquire a session: Session.login_with_password()\n\n\n\n\n\n\nOne call to query the VM (and template) objects present on the XenServer installation: VM.get_all_records(). Recall that we used the information returned from this call to select a suitable template to install from.\n\n\n\n\n\n\nTwo calls to install a VM from our chosen template: VM.clone(), followed by VM.provision().\n\n\n\n\n\n\nOne call to start the resultant VM: VM.start() (and similarly other single calls to suspend, resume and shutdown accordingly)\n\n\n\n\n\n\nAnd then one call to logout Session.logout()\n\n\n\n\n\n\nThe take-home message here is that, although the API as a whole is complex and fully featured, common tasks (such as creating and performing lifecycle operations on VMs) are very straightforward to perform, requiring only a small number of simple API calls. Keep this in mind while you study the next section which may, on first reading, appear a little daunting!\n\n\n3.2. Object Model Overview\n\n\nThis section gives a high-level overview of the object model of the API. A more detailed description of the parameters and methods of each class outlined here can be found in the XenServer API Reference document.\n\n\nWe start by giving a brief outline of some of the core classes that make up the API. (Don't worry if these definitions seem somewhat abstract in their initial presentation; the textual description in subsequent sections, and the code-sample walk through in the next Chapter will help make these concepts concrete.)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVM\n\n\nA VM object represents a particular virtual machine instance on a XenServer Host or Resource Pool. Example methods include start, suspend, pool_migrate; example parameters include power_state, memory_static_max, and name_label. (In the previous section we saw how the VM class is used to represent both templates and regular VMs)\n\n\n\n\n\n\nHost\n\n\nA host object represents a physical host in a XenServer pool. Example methods include reboot and shutdown. Example parameters include software_version, hostname, and [IP] address.\n\n\n\n\n\n\nVDI\n\n\nA VDI object represents a Virtual Disk Image. Virtual Disk Images can be attached to VMs, in which case a block device appears inside the VM through which the bits encapsulated by the Virtual Disk Image can be read and written. Example methods of the VDI class include \"resize\" and \"clone\". Example fields include \"virtual_size\" and \"sharable\". (When we called VM.provision on the VM template in our previous example, some VDI objects were automatically created to represent the newly created disks, and attached to the VM object.)\n\n\n\n\n\n\nSR\n\n\nAn SR (Storage Repository) aggregates a collection of VDIs and encapsulates the properties of physical storage on which the VDIs' bits reside. Example parameters include type (which determines the storage-specific driver a XenServer installation uses to read/write the SR's VDIs) and physical_utilisation; example methods include scan(which invokes the storage-specific driver to acquire a list of the VDIs contained with the SR and the properties of these VDIs) and create (which initializes a block of physical storage so it is ready to store VDIs).\n\n\n\n\n\n\nNetwork\n\n\nA network object represents a layer-2 network that exists in the environment in which the XenServer Host instance lives. Since XenServer does not manage networks directly this is a lightweight class that serves merely to model physical and virtual network topology. VM and Host objects that are attached to a particular Network object (by virtue of VIF and PIF instances -- see below) can send network packets to each other.\n\n\n\n\n\n\n\n\nAt this point, readers who are finding this enumeration of classes rather terse may wish to skip to the code walk-throughs of the next chapter: there are plenty of useful applications that can be written using only a subset of the classes already described! For those who wish to continue this description of classes in the abstract, read on.\n\n\nOn top of the classes listed above, there are 4 more that act as connectors, specifying relationships between VMs and Hosts, and Storage and Networks. The first 2 of these classes that we will consider, VBD and VIF, determine how VMs are attached to virtual disks and network objects respectively:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVBD\n\n\nA VBD (Virtual Block Device) object represents an attachment between a VM and a VDI. When a VM is booted its VBD objects are queried to determine which disk images (VDIs) should be attached. Example methods of the VBD class include \"plug\" (which hot plugs a disk device into a running VM, making the specified VDI accessible therein) and \"unplug\" (which hot unplugs a disk device from a running guest); example fields include \"device\" (which determines the device name inside the guest under which the specified VDI will be made accessible).\n\n\n\n\n\n\nVIF\n\n\nA VIF (Virtual network InterFace) object represents an attachment between a VM and a Network object. When a VM is booted its VIF objects are queried to determine which network devices should be created. Example methods of the VIF class include \"plug\" (which hot plugs a network device into a running VM) and \"unplug\" (which hot unplugs a network device from a running guest).\n\n\n\n\n\n\n\n\nThe second set of \"connector classes\" that we will consider determine how Hosts are attached to Networks and Storage.\n|||\n|-|-|\n| PIF  | A PIF (Physical InterFace) object represents an attachment between a Host and a Network object. If a host is connected to a Network (over a PIF) then packets from the specified host can be transmitted/received by the corresponding host. Example fields of the PIF class include \"device\" (which specifies the device name to which the PIF corresponds -- e.g. eth0) and \"MAC\" (which specifies the MAC address of the underlying NIC that a PIF represents). Note that PIFs abstract both physical interfaces and VLANs (the latter distinguished by the existence of a positive integer in the \"VLAN\" field).|\n| PBD   | A PBD (Physical Block Device) object represents an attachment between a Host and a SR (Storage Repository) object. Fields include \"currently-attached\" (which specifies whether the chunk of storage represented by the specified SR object) is currently available to the host; and \"device_config\" (which specifies storage-driver specific parameters that determines how the low-level storage devices are configured on the specified host -- e.g. in the case of an SR rendered on an NFS filer, device_config may specify the host-name of the filer and the path on the filer in which the SR files live.)|\n\n\nFigure 3.1. Common API Classes\n\n\n\n\nGraphical overview of API classes for managing VMs, Hosts, Storage and Networking\n\n\nFigure 3.1, \u00a1\u00b0Common API Classes\" presents a graphical overview of the API classes involved in managing VMs, Hosts, Storage and Networking. From this diagram, the symmetry between storage and network configuration, and also the symmetry between virtual machine and host configuration is plain to see.\n\n\n3.3. Working with VIFs and VBDs\n\n\nIn this section we walk through a few more complex scenarios, describing informally how various tasks involving virtual storage and network devices can be accomplished using the API.\n\n\n3.3.1. Creating disks and attaching them to VMs\n\n\nLet's start by considering how to make a new blank disk image and attach it to a running VM. We will assume that we already have ourselves a running VM, and we know its corresponding API object reference (e.g. we may have created this VM using the procedure described in the previous section, and had the server return its reference to us.) We will also assume that we have authenticated with the XenServer installation and have a corresponding session reference. Indeed in the rest of this chapter, for the sake of brevity, we will stop mentioning sessions altogether.\n\n\n3.3.1.1. Creating a new blank disk image\n\n\nThe first step is to instantiate the disk image on physical storage. We do this by calling VDI.create(). The VDI.create call takes a number of parameters, including:\n\n\n\n\n\n\nname_label and name_description: a human-readable name/description for the disk (e.g. for convenient display in the UI etc.). These fields can be left blank if desired.\n\n\n\n\n\n\nSR: the object reference of the Storage Repository representing the physical storage in which the VDI's bits will be placed.\n\n\n\n\n\n\nread_only: setting this field to true indicates that the VDI can only be attached to VMs in a read-only fashion. (Attempting to attach a VDI with its read_only field set to true in a read/write fashion results in error.)\n\n\n\n\n\n\nInvoking the VDI.create call causes the XenServer installation to create a blank disk image on physical storage, create an associated VDI object (the datamodel instance that refers to the disk image on physical storage) and return a reference to this newly created VDI object.\n\n\nThe way in which the disk image is represented on physical storage depends on the type of the SR in which the created VDI resides. For example, if the SR is of type \"lvm\" then the new disk image will be rendered as an LVM volume; if the SR is of type \"nfs\" then the new disk image will be a sparse VHD file created on an NFS filer. (You can query the SR type through the API using the SR.get_type() call.)\n\n\n\n\nNote\n\nSome SR types might round up the virtual-size value to make it divisible by a configured block size.\n\n\n\n\n3.3.1.2. Attaching the disk image to a VM\n\n\nSo far we have a running VM (that we assumed the existence of at the start of this example) and a fresh VDI that we just created. Right now, these are both independent objects that exist on the XenServer Host, but there is nothing linking them together. So our next step is to create such a link, associating the VDI with our VM.\n\n\nThe attachment is formed by creating a new \"connector\" object called a VBD (Virtual Block Device). To create our VBD we invoke the VBD.create() call. The VBD.create() call takes a number of parameters including:\n\n\n\n\n\n\nVM - the object reference of the VM to which the VDI is to be attached\n\n\n\n\n\n\nVDI - the object reference of the VDI that is to be attached\n\n\n\n\n\n\nmode - specifies whether the VDI is to be attached in a read-only or a read-write fashion\n\n\n\n\n\n\nuserdevice - specifies the block device inside the guest through which applications running inside the VM will be able to read/write the VDI's bits.\n\n\n\n\n\n\ntype - specifies whether the VDI should be presented inside the VM as a regular disk or as a CD. (Note that this particular field has more meaning for Windows VMs than it does for Linux VMs, but we will not explore this level of detail in this chapter.)\n\n\n\n\n\n\nInvoking VBD.create makes a VBD object on the XenServer installation and returns its object reference. However, this call in itself does not have any side-effects on the running VM (that is, if you go and look inside the running VM you will see that the block device has not been created). The fact that the VBD object exists but that the block device in the guest is not active, is reflected by the fact that the VBD object's currently_attached field is set to false.\n\n\nFigure 3.2. A VM object with 2 associated VDIs\n\n\n\n\nA VM object with 2 associated VDIs\n\n\nFor expository purposes, Figure 3.2, \u00a1\u00b0A VM object with 2 associated VDIs??? presents a graphical example that shows the relationship between VMs, VBDs, VDIs and SRs. In this instance a VM object has 2 attached VDIs: there are 2 VBD objects that form the connections between the VM object and its VDIs; and the VDIs reside within the same SR.\n\n\n3.3.1.3. Hotplugging the VBD\n\n\nIf we rebooted the VM at this stage then, after rebooting, the block device corresponding to the VBD would appear: on boot, XenServer queries all VBDs of a VM and actively attaches each of the corresponding VDIs.\n\n\nRebooting the VM is all very well, but recall that we wanted to attach a newly created blank disk to a running VM. This can be achieved by invoking the plug method on the newly created VBD object. When the plug call returns successfully, the block device to which the VBD relates will have appeared inside the running VM -- i.e. from the perspective of the running VM, the guest operating system is led to believe that a new disk device has just been hot plugged. Mirroring this fact in the managed world of the API, the currently_attached field of the VBD is set to true.\n\n\nUnsurprisingly, the VBD plug method has a dual called \"unplug\". Invoking the unplug method on a VBD object causes the associated block device to be hot unplugged from a running VM, setting the currently_attached field of the VBD object to false accordingly.\n\n\n3.3.2. Creating and attaching Network Devices to VMs\n\n\nThe API calls involved in configuring virtual network interfaces in VMs are similar in many respects to the calls involved in configuring virtual disk devices. For this reason we will not run through a full example of how one can create network interfaces using the API object-model; instead we will use this section just to outline briefly the symmetry between virtual networking device and virtual storage device configuration.\n\n\nThe networking analogue of the VBD class is the VIF class. Just as a VBD is the API representation of a block device inside a VM, a VIF (Virtual network InterFace) is the API representation of a network device inside a VM. Whereas VBDs associate VM objects with VDI objects, VIFs associate VM objects with Network objects. Just like VBDs, VIFs have a currently_attached field that determines whether or not the network device (inside the guest) associated with the VIF is currently active or not. And as we saw with VBDs, at VM boot-time the VIFs of the VM are queried and a corresponding network device for each created inside the booting VM. Similarly, VIFs also have plug and unplug methods for hot plugging/unplugging network devices in/out of running VMs.\n\n\n3.3.3. Host configuration for networking and storage\n\n\nWe have seen that the VBD and VIF classes are used to manage configuration of block devices and network devices (respectively) inside VMs. To manage host configuration of storage and networking there are two analogous classes: PBD (Physical Block Device) and PIF (Physical [network] InterFace).\n\n\n3.3.3.1. Host storage configuration: PBDs\n\n\nLet us start by considering the PBD class. A PBD_create() call takes a number of parameters including:\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nhost\n\n\nphysical machine on which the PBD is available\n\n\n\n\n\n\nSR\n\n\nthe Storage Repository that the PBD connects to\n\n\n\n\n\n\ndevice_config\n\n\na string-to-string map that is provided to the host's SR-backend-driver, containing the low-level parameters required to configure the physical storage device(s) on which the SR is to be realized. The specific contents of the device_config field depend on the type of the SR to which the PBD is connected. (Executing xe sm-listwill show a list of possible SR types; the configuration field in this enumeration specifies the device_config parameters that each SR type expects.)\n\n\n\n\n\n\n\n\nFor example, imagine we have an SR object s of type \"nfs\" (representing a directory on an NFS filer within which VDIs are stored as VHD files); and let's say that we want a host, h, to be able to access s. In this case we invoke PBD.create() specifying host h, SR s, and a value for the device_config parameter that is the following map:\n\n\n(\"server\", \"my_nfs_server.example.com\"), (\"serverpath\", \"/scratch/mysrs/sr1\")\n\n\nThis tells the XenServer Host that SR s is accessible on host h, and further that to access SR s, the host needs to mount the directory /scratch/mysrs/sr1 on the NFS server named my_nfs_server.example.com.\n\n\nLike VBD objects, PBD objects also have a field called currently_attached. Storage repositories can be attached and detached from a given host by invoking PBD.plug and PBD.unplug methods respectively.\n\n\n3.3.3.2. Host networking configuration: PIFs\n\n\nHost network configuration is specified by virtue of PIF objects. If a PIF object connects a network object, n, to a host object h, then the network corresponding to n is bridged onto a physical interface (or a physical interface plus a VLAN tag) specified by the fields of the PIF object.\n\n\nFor example, imagine a PIF object exists connecting host h to a network n, and that device field of the PIF object is set to eth0. This means that all packets on network n are bridged to the NIC in the host corresponding to host network device eth0.\n\n\n3.4. Exporting and Importing VMs\n\n\nVMs can be exported to a file and later imported to any XenServer host. The export protocol is a simple HTTP(S) GET, which should be performed on the master if the VM is on a pool member. Authorization is either standard HTTP basic authentication, or if a session has already been obtained, this can be used. The VM to export is specified either by UUID or by reference. To keep track of the export, a task can be created and passed in using its reference. The request might result in a redirect if the VM's disks are only accessible on a pool member.\n\n\nThe following arguments are passed on the command line:\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nsession_id\n\n\nthe reference of the session being used to authenticate; required only when not using HTTP basic authentication\n\n\n\n\n\n\ntask_id\n\n\nthe reference of the task object with which to keep track of the operation; optional, required only if you have created a task object to keep track of the export\n\n\n\n\n\n\nref\n\n\nthe reference of the VM; required only if not using the UUID\n\n\n\n\n\n\nuuid\n\n\nthe UUID of the VM; required only if not using the reference\n\n\n\n\n\n\n\n\nFor example, using the Linux command line tool cURL:\n\n\ncurl http://root:foo@myxenserver1/export?uuid=\nvm_uuid\n -o \nexportfile\n\n\nwill export the specified VM to the file exportfile.\n\n\nTo export just the metadata, use the URI http://server/export_metadata.\n\n\nThe import protocol is similar, using HTTP(S) PUT. The session_id and task_id arguments are as for the export. The ref and uuid are not used; a new reference and uuid will be generated for the VM. There are some additional parameters:\n\n\n\n\n\n\n\n\nArgument\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\nrestore\n\n\nif true, the import is treated as replacing the original VM - the implication of this currently is that the MAC addresses on the VIFs are exactly as the export was, which will lead to conflicts if the original VM is still being run.\n\n\n\n\n\n\nforce\n\n\nif true, any checksum failures will be ignored (the default is to destroy the VM if a checksum error is detected)\n\n\n\n\n\n\nsr_id\n\n\nthe reference of an SR into which the VM should be imported. The default behavior is to import into the Pool.default_SR.\n\n\n\n\n\n\n\n\nFor example, again using cURL:\n\n\ncurl -T \nexportfile\n http://root:foo@myxenserver2/import\n\nwill import the VM to the default SR on the server.\n\n\n\n\nNote\n\nNote that if no default SR has been set, and no sr_uuid is specified, the error message \"DEFAULT_SR_NOT_FOUND\" is returned\n\n\n\n\nAnother example:\n\n\ncurl -T \nexportfile\n http://root:foo@myxenserver2/import?sr_id=\nopaque_ref_of_sr\n\nwill import the VM to the specified SR on the server.\n\n\nTo import just the metadata, use the URI http://server/import_metadata\n\n\n3.4.1. Xen Virtual Appliance (XVA) VM Import Format\n\n\nXenServer supports a human-readable legacy VM input format called XVA. This section describes the syntax and structure of XVA.\n\n\nAn XVA consists of a directory containing XML metadata and a set of disk images. A VM represented by an XVA is not intended to be directly executable. Data within an XVA package is compressed and intended for either archiving on permanent storage or for being transmitted to a VM server - such as a XenServer host - where it can be decompressed and executed.\n\n\nXVA is a hypervisor-neutral packaging format; it should be possible to create simple tools to instantiate an XVA VM on any other platform. XVA does not specify any particular runtime format; for example disks may be instantiated as file images, LVM volumes, QCoW images, VMDK or VHD images. An XVA VM may be instantiated any number of times, each instantiation may have a different runtime format.\n\n\nXVA does not:\n\n\n\n\n\n\nspecify any particular serialization or transport format\n\n\n\n\n\n\nprovide any mechanism for customizing VMs (or templates) on install\n\n\n\n\n\n\naddress how a VM may be upgraded post-install\n\n\n\n\n\n\ndefine how multiple VMs, acting as an appliance, may communicate\n\n\n\n\n\n\nThese issues are all addressed by the related Open Virtual Appliance specification.\n\n\nAn XVA is a directory containing, at a minimum, a file called ova.xml. This file describes the VM contained within the XVA and is described in Section 3.2. Disks are stored within sub-directories and are referenced from the ova.xml. The format of disk data is described later in Section 3.3.\n\n\nThe following terms will be used in the rest of the chapter:\n\n\n\n\n\n\nHVM: a mode in which unmodified OS kernels run with the help of virtualization support in the hardware.\n\n\n\n\n\n\nPV: a mode in which specially modified \"paravirtualized\" kernels run explicitly on top of a hypervisor without requiring hardware support for virtualization.\n\n\n\n\n\n\nThe \n\"ova.xml\"\n file contains the following elements:\n\n\nappliance version=\"0.1\"\n\nThe number in the attribute \"version\" indicates the version of this specification to which the XVA is constructed; in this case version 0.1. Inside the \n there is exactly one \n: (in the OVA specification, multiple \ns are permitted)\n\n\nvm name=\"name\"\n\nEach \n element describes one VM. The \"name\" attribute is for future internal use only and must be unique within the ova.xml file. The \"name\" attribute is permitted to be any valid UTF-8 string. Inside each \n tag are the following compulsory elements:\n\n\nlabel\n... text ... \n/label\n\nA short name for the VM to be displayed in a UI.\n\n\nshortdesc\n ... description ... \n/shortdesc\n\nA description for the VM to be displayed in the UI. Note that for both \nlabel\n and \nshortdesc\n contents, leading and trailing whitespace will be ignored.\n\n\nconfig mem_set=\"268435456\" vcpus=\"1\"/\n\nThe \n element has attributes which describe the amount of memory in bytes (mem_set) and number of CPUs (VCPUs) the VM should have.\n\n\nEach \n has zero or more \nvbd\n elements representing block devices which look like the following:\n\n\nvbd device=\"sda\" function=\"root\" mode=\"w\" vdi=\"vdi_sda\"/\n\nThe attributes have the following meanings:\n\n\n\n\n\n\ndevice\n    name of the physical device to expose to the VM. For linux guests we use \"sd[a-z]\" and for windows guests we use \"hd[a-d]\".\n\n\n\n\n\n\nfunction\nif marked as \"root\", this disk will be used to boot the guest. (NB this does not imply the existence of the Linux root i.e. / filesystem) Only one device should be marked as \"root\". See Section 3.4 describing VM booting. Any other string is ignored.\n\n\n\n\n\n\nmode\neither \"w\" or \"ro\" if the device is to be read/write or read-only\n\n\n\n\n\n\nvdi\nthe name of the disk image (represented by a \n element) to which this block device is connected\n\n\n\n\n\n\nEach \nvm\n may have an optional \nhacks\n section like the following: \nhacks is_hvm=\"false\" kernel_boot_cmdline=\"root=/dev/sda1 ro\"/\n The \nhacks\n element is present in the XVA files generated by XenServer but will be removed in future. The attribute \"is_hvm\" is either \"true\" or \"false\", depending on whether the VM should be booted in HVM or not. The \"kernel_boot_cmdline\" contains additional kernel commandline arguments when booting a guest using pygrub.\n\n\nIn addition to a \nvm\n element, the \nappliance\n will contain zero or more \nvdi\n elements like the following:\n\n\nvdi name=\"vdi_sda\" size=\"5368709120\" source=\"file://sda\"\ntype=\"dir-gzipped-chunks\"\n\nEach \nvdi\n corresponds to a disk image. The attributes have the following meanings:\n\n\nname: name of the VDI, referenced by the vdi attribute of \nvbd\n elements. Any valid UTF-8 string is permitted.\n\n\nsize: size of the required image in bytes\n\n\nsource: a URI describing where to find the data for the image, only file:// URIs are currently permitted and must describe paths relative to the directory containing the ova.xml\n\n\ntype: describes the format of the disk data (see Section 3.3)\n\n\nA single disk image encoding is specified in which has type \"dir-gzipped-chunks\": Each image is represented by a directory containing a sequence of files as follows:\n\n\n-rw-r--r-- 1 dscott xendev 458286013    Sep 18 09:51 chunk000000000.gz\n-rw-r--r-- 1 dscott xendev 422271283    Sep 18 09:52 chunk000000001.gz\n-rw-r--r-- 1 dscott xendev 395914244    Sep 18 09:53 chunk000000002.gz\n-rw-r--r-- 1 dscott xendev 9452401      Sep 18 09:53 chunk000000003.gz\n-rw-r--r-- 1 dscott xendev 1096066      Sep 18 09:53 chunk000000004.gz\n-rw-r--r-- 1 dscott xendev 971976       Sep 18 09:53 chunk000000005.gz\n-rw-r--r-- 1 dscott xendev 971976       Sep 18 09:53 chunk000000006.gz\n-rw-r--r-- 1 dscott xendev 971976       Sep 18 09:53 chunk000000007.gz\n-rw-r--r-- 1 dscott xendev 573930       Sep 18 09:53 chunk000000008.gz\n\n\nEach file (named \n\"chunk-XXXXXXXXX.gz\"\n) is a gzipped file containing exactly 1e9 bytes (1GB, not 1GiB) of raw block data. The small size was chosen to be safely under the maximum file size limits of several filesystems. If the files are gunzipped and then concatenated together, the original image is recovered.\n\n\nXenServer provides two mechanisms for booting a VM: (i) using a paravirtualized kernel extracted through pygrub; and (ii) using HVM. The current implementation uses the \"is_hvm\" flag within the \n section to decide which mechanism to use.\n\n\nThis rest of this section describes a very simple Debian VM packaged as an XVA. The VM has two disks, one with size 5120MiB and used for the root filesystem and used to boot the guest using pygrub and the other of size 512MiB which is used for swap. The VM has 512MiB of memory and uses one virtual CPU.\n\n\nAt the topmost level the simple Debian VM is represented by a single directory:\n\n\n$ ls -l\ntotal 4\ndrwxr-xr-x 3 dscott xendev 4096 Oct 24 09:42 very simple Debian VM\n\n\nInside the main XVA directory are two sub-directories - one per disk - and the single file: ova.xml:\n\n\n$ ls -l very\\ simple\\ Debian\\ VM/\ntotal 8\n-rw-r--r-- 1 dscott xendev 1016 Oct 24 09:42 ova.xml\ndrwxr-xr-x 2 dscott xendev 4096 Oct 24 09:42 sda\ndrwxr-xr-x 2 dscott xendev 4096 Oct 24 09:53 sdb\n\n\nInside each disk sub-directory are a set of files, each file contains 1GB of raw disk block data compressed using gzip:\n\n\n$ ls -l very\\ simple\\ Debian\\ VM/sda/\ntotal 2053480\n-rw-r--r-- 1 dscott xendev 202121645 Oct 24 09:43 chunk-000000000.gz\n-rw-r--r-- 1 dscott xendev 332739042 Oct 24 09:45 chunk-000000001.gz\n-rw-r--r-- 1 dscott xendev 401299288 Oct 24 09:48 chunk-000000002.gz\n-rw-r--r-- 1 dscott xendev 389585534 Oct 24 09:50 chunk-000000003.gz\n-rw-r--r-- 1 dscott xendev 624567877 Oct 24 09:53 chunk-000000004.gz\n-rw-r--r-- 1 dscott xendev 150351797 Oct 24 09:54 chunk-000000005.gz\n$ ls -l very\\ simple\\ Debian\\ VM/sdb\ntotal 516\n-rw-r--r-- 1 dscott xendev 521937 Oct 24 09:54 chunk-000000000.gz\n\n\nThe example simple Debian VM would have an XVA file like the following:\n\n\n?xml version=\"1.0\" ?\n\n\nappliance version=\"0.1\"\n\n  \nvm name=\"vm\"\n\n  \nlabel\n\n  very simple Debian VM\n  \n/label\n\n  \nshortdesc\n\n  the description field can contain any valid UTF-8\n  \n/shortdesc\n\n  \nconfig mem_set=\"536870912\" vcpus=\"1\"/\n\n  \nhacks is_hvm=\"false\" kernel_boot_cmdline=\"root=/dev/sda1 ro \"\n\n  \n!--This section is temporary and will be ignored in future. Attribute\nis_hvm (\"true\" or \"false\") indicates whether the VM will be booted in HVM mode. In\nfuture this will be autodetected. Attribute kernel_boot_cmdline contains the kernel\ncommandline for the case where a proper grub menu.lst is not present. In future\nbooting shall only use pygrub.--\n\n  \n/hacks\n\n  \nvbd device=\"sda\" function=\"root\" mode=\"w\" vdi=\"vdi_sda\"/\n\n  \nvbd device=\"sdb\" function=\"swap\" mode=\"w\" vdi=\"vdi_sdb\"/\n\n  \n/vm\n\n  \nvdi name=\"vdi_sda\" size=\"5368709120\" source=\"file://sda\" type=\"dir-gzippedchunks\"/\n\n  \nvdi name=\"vdi_sdb\" size=\"536870912\" source=\"file://sdb\" type=\"dir-gzippedchunks\"/\n\n\n/appliance\n\n\n3.5. XML-RPC notes\n\n\n3.5.1. Datetimes\n\n\nThe API deviates from the XML-RPC specification in handling of datetimes. The API appends a \"Z\" to the end of datetime strings, which is meant to indicate that the time is expressed in UTC.\n\n\n3.6. Where to look next\n\n\nIn this chapter we have presented a brief high-level overview of the API and its object-model. The aim here is not to present the detailed semantics of the API, but just to provide enough background for you to start reading the code samples of the next chapter and to find your way around the more detailed XenServer API Reference reference document.\n\n\nThere are a number of places you can find more information:\n\n\nThe XenServer Administrators Guide contains an overview of the xe CLI. Since a good deal of xe commands are a thin veneer over the API, playing with xe is a good way to start finding your way around the API object model described in this chapter.\n\n\nThe code samples in the next chapter provide some concrete instances of API coding in a variety of client languages.\n\n\nThe \nXenServer API Reference reference\n document provides a more detailed description of the API semantics as well as describing the format of XML/RPC messages on the wire.\n\n\nThere are a few scripts that use the API in the XenServer Host dom0 itself. For example, \"/opt/xensource/libexec/shutdown\" is a python program that cleanly shuts VMs down. This script is invoked when the host itself is shut down.\n\n\nChapter 4. Using the API\n\n\nThis chapter describes how to use the XenServer Management API from real programs to manage XenServer Hosts and VMs. The chapter begins with a walk-through of a typical client application and demonstrates how the API can be used to perform common tasks. Example code fragments are given in python syntax but equivalent code in the other programming languages would look very similar. The language bindings themselves are discussed afterwards and the chapter finishes with walk-throughs of two complete examples.\n\n\n4.1. Anatomy of a typical application\n\n\nThis section describes the structure of a typical application using the XenServer Management API. Most client applications begin by connecting to a XenServer Host and authenticating (e.g. with a username and password). Assuming the authentication succeeds, the server will create a \"session\" object and return a reference to the client. This reference will be passed as an argument to all future API calls. Once authenticated, the client may search for references to other useful objects (e.g. XenServer Hosts, VMs, etc.) and invoke operations on them. Operations may be invoked either synchronously or asynchronously; special task objects represent the state and progress of asynchronous operations. These application elements are all described in detail in the following sections.\n\n\n4.1.1. Choosing a low-level transport\n\n\nAPI calls can be issued over two transports:\n\n\n\n\n\n\nSSL-encrypted TCP on port 443 (https) over an IP network\n\n\n\n\n\n\nplaintext over a local Unix domain socket: /var/xapi/xapi\n\n\n\n\n\n\nThe SSL-encrypted TCP transport is used for all off-host traffic while the Unix domain socket can be used from services running directly on the XenServer Host itself. In the SSL-encrypted TCP transport, all API calls should be directed at the Resource Pool master; failure to do so will result in the error HOST_IS_SLAVE, which includes the IP address of the master as an error parameter.\n\n\nBecause the master host of a pool can change, especially if HA is enabled on a pool, clients must implement the following steps to detect a master host change and connect to the new master as required:\n\n\nHandling pool master changes\n\n\n\n\n\n\nSubscribe to updates in the list of hosts servers, and maintain a current list of hosts in the pool\n\n\n\n\n\n\nIf the connection to the pool master fails to respond, attempt to connect to all hosts in the list until one responds\n\n\n\n\n\n\nThe first host to respond will return the HOST_IS_SLAVE error message, which contains the identity of the new pool master (unless of course the host is the new master)\n\n\n\n\n\n\nConnect to the new master\n\n\n\n\n\n\n\n\nNote\n\nAs a special-case, all messages sent through the Unix domain socket are transparently forwarded to the correct node.\n\n\n\n\n4.1.2. Authentication and session handling\n\n\nThe vast majority of API calls take a session reference as their first parameter; failure to supply a valid reference will result in a SESSION_INVALID error being returned. Acquire a session reference by supplying a username and password to the login_with_password function.\n\n\n\n\nNote\n\nAs a special-case, if this call is executed over the local Unix domain socket then the username and password are ignored and the call always succeeds\n\n\n\n\nEvery session has an associated \"last active\" timestamp which is updated on every API call. The server software currently has a built-in limit of 500 active sessions and will remove those with the oldest \"last active\" field if this limit is exceeded for a given username or originator. In addition all sessions whose \"last active\" field is older than 24 hours are also removed. Therefore it is important to:\n\n\n\n\n\n\nSpecify an appropriate originator when logging in; and\n\n\n\n\n\n\nRemember to log out of active sessions to avoid leaking them; and\n\n\n\n\n\n\nBe prepared to log in again to the server if a SESSION_INVALID error is caught.\n\n\n\n\n\n\nIn the following Python fragment a connection is established over the Unix domain socket and a session is created:\n\n\nimport\n \nXenAPI\n\n\n    \nsession\n \n=\n \nXenAPI\n.\nxapi_local\n()\n\n    \ntry\n:\n\n        \nsession\n.\nxenapi\n.\nlogin_with_password\n(\nroot\n,\n \n,\n \n2.3\n,\n \nMy Widget v0.1\n)\n\n        \n...\n\n    \nfinally\n:\n\n        \nsession\n.\nxenapi\n.\nsession\n.\nlogout\n()\n\n\n\n\n\n\n4.1.3. Finding references to useful objects\n\n\nOnce an application has authenticated the next step is to acquire references to objects in order to query their state or invoke operations on them. All objects have a set of \"implicit\" messages which include the following:\n\n\n\n\n\n\nget_by_name_label : return a list of all objects of a particular class with a particular label;\n\n\n\n\n\n\nget_by_uuid : return a single object named by its UUID;\n\n\n\n\n\n\nget_all : return a set of references to all objects of a particular class; and\n\n\n\n\n\n\nget_all_records : return a map of reference to records for each object of a particular class.\n\n\n\n\n\n\nFor example, to list all hosts:\n\n\nhosts = session.xenapi.host.get_all()\n\nTo find all VMs with the name \"my first VM\":\n\n\nvms = session.xenapi.VM.get_by_name_label('my first VM')\n\n\n\n\nNote\n\nObject name_label fields are not guaranteed to be unique and so the get_by_name_label API call returns a set of references rather than a single reference.\n\n\n\n\nIn addition to the methods of finding objects described above, most objects also contain references to other objects within fields. For example it is possible to find the set of VMs running on a particular host by calling:\n\n\nvms = session.xenapi.host.get_resident_VMs(host)\n\n\n4.1.4. Invoking synchronous operations on objects\n\n\nOnce object references have been acquired, operations may be invoked on them. For example to start a VM:\n\n\nsession.xenapi.VM.start(vm, False, False)\n\nAll API calls are by default synchronous and will not return until the operation has completed or failed. For example in the case of VM.start the call does not return until the VM has started booting.\n\n\n\n\nNote\n\nWhen the VM.start call returns the VM will be booting. To determine when the booting has finished, wait for the in-guest agent to report internal statistics through the VM_guest_metrics object.\n\n\n\n\n4.1.5. Using Tasks to manage asynchronous operations\n\n\nTo simplify managing operations which take quite a long time (e.g. VM.clone and VM.copy) functions are available in two forms: synchronous (the default) and asynchronous. Each asynchronous function returns a reference to a task object which contains information   about the in-progress operation including:\n\n\n\n\n\n\nwhether it is pending\n\n\n\n\n\n\nwhether it is has succeeded or failed\n\n\n\n\n\n\nprogress (in the range 0-1)\n\n\n\n\n\n\nthe result or error code returned by the operation\n\n\n\n\n\n\nAn application which wanted to track the progress of a VM.clone operation and display a progress bar would have code like the following:\n\n\nvm = session.xenapi.VM.get_by_name_label(\nmy vm\n)\ntask = session.xenapi.Async.VM.clone(vm)\nwhile session.xenapi.task.get_status(task) == \npending\n:\n        progress = session.xenapi.task.get_progress(task)\n        update_progress_bar(progress)\n        time.sleep(1)\nsession.xenapi.task.destroy(task)\n\n\n\n\n\n\n\nNote\n\nNote that a well-behaved client should remember to delete tasks created by asynchronous operations when it has finished reading the result or error. If the number of tasks exceeds a built-in threshold then the server will delete the oldest of the completed tasks.\n\n\n\n\n4.1.6. Subscribing to and listening for events\n\n\nWith the exception of the task and metrics classes, whenever an object is modified the server generates an event. Clients can subscribe to this event stream on a per-class basis and receive updates rather than resorting to frequent polling. Events come in three types:\n\n\n\n\n\n\nadd - generated when an object has been created;\n\n\n\n\n\n\ndel - generated immediately before an object is destroyed; and\n\n\n\n\n\n\nmod - generated when an object's field has changed.\n\n\n\n\n\n\nEvents also contain a monotonically increasing ID, the name of the class of object and a snapshot of the object state equivalent to the result of a get_record().\n\n\nClients register for events by calling event.register() with a list of class names or the special string \"*\". Clients receive events by executing event.next() which blocks until events are available and returns the new events.\n\n\n\n\nNote\n\nSince the queue of generated events on the server is of finite length a very slow client might fail to read the events fast enough; if this happens an EVENTS_LOST error is returned. Clients should be prepared to handle this by re-registering for events and checking that the condition they are waiting for hasn't become true while they were unregistered.\n\n\n\n\nThe following python code fragment demonstrates how to print a summary of every event generated by a system: (similar code exists in \nXenserver-SDK/XenServerPython/samples/watch-all-events.py\n)\n\n\nfmt = \n%8s  %20s  %5s  %s\n\nsession.xenapi.event.register([\n*\n])\nwhile True:\n    try:\n        for event in session.xenapi.event.next():\n            name = \n(unknown)\n\n            if \nsnapshot\n in event.keys():\n                snapshot = event[\nsnapshot\n]\n                if \nname_label\n in snapshot.keys():\n                    name = snapshot[\nname_label\n]\n            print fmt % (event[\nid\n], event[\nclass\n], event[\noperation\n], name)           \n    except XenAPI.Failure, e:\n        if e.details == [ \nEVENTS_LOST\n ]:\n            print \nCaught EVENTS_LOST; should reregister\n\n\n\n\n\n\n4.2. Language bindings\n\n\n4.2.1. C\n\n\nThe SDK includes the source to the C language binding in the directory XenServer-SDK/libxenserver/src together with a Makefile which compiles the binding into a library. Every API object is associated with a header file which contains declarations for all that object's API functions; for example the type definitions and functions required to invoke VM operations are all contained in xen_vm.h.\n\n\nC binding dependencies\n\n|||\n-------------|---------------|\n|Platform|supported:    Linux|\n|Library:|The language binding is generated as a libxenserver.so that is linked by C programs.|\n|Dependencies:|\nXML library (libxml2.so on GNU Linux)\nCurl library (libcurl2.so)\n|\n\n\nThe following simple examples are included with the C bindings:\n\n\n\n\n\n\ntest_vm_async_migrate: demonstrates how to use asynchronous API calls to migrate running VMs from a slave host to the pool master.\n\n\n\n\n\n\ntest_vm_ops: demonstrates how to query the capabilities of a host, create a VM, attach a fresh blank disk image to the VM and then perform various powercycle operations;\n\n\n\n\n\n\ntest_failures: demonstrates how to translate error strings into enum_xen_api_failure, and vice versa;\n\n\n\n\n\n\ntest_event_handling: demonstrates how to listen for events on a connection.\n\n\n\n\n\n\ntest_enumerate: demonstrates how to enumerate the various API objects.\n\n\n\n\n\n\n4.2.2. C\n\n\nThe C# bindings are contained within the directory XenServer-SDK/XenServer.NET and include project files suitable for building under Microsoft Visual Studio. Every API object is associated with one C# file; for example the functions implementing the VM operations are contained within the file VM.cs.\n\n\nC# binding dependencies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlatform supported:\n\n\nWindows with .NET version 4.5\n\n\n\n\n\n\nLibrary:\n\n\nThe language binding is generated as a Dynamic Link Library XenServer.dll that is linked by C# programs.\n\n\n\n\n\n\nDependencies:\n\n\nCookComputing.XMLRpcV2.dll is needed for the XenServer.dll to be able to communicate with the xml-rpc server. We ship a patched 2.1.0.6 version and recommend that you use this one, though others may work.\n\n\n\n\n\n\n\n\nThree examples are included with the C# bindings in the directory XenServer-SDK/XneServer.NET/samples as separate projects of the XenSdkSample.sln solution:\n\n\n\n\n\n\nGetVariousRecords: logs into a XenServer Host and displays information about hosts, storage and virtual machines;\n\n\n\n\n\n\nGetVmRecords: logs into a XenServer Host and lists all the VM records;\n\n\n\n\n\n\nVmPowerStates: logs into a XenServer Host, finds a VM and takes it through the various power states. Requires a shut-down VM to be already installed.\n\n\n\n\n\n\n4.2.3. Java\n\n\nThe Java bindings are contained within the directory XenServer-SDK/XenServerJava and include project files suitable for building under Microsoft Visual Studio. Every API object is associated with one Java file; for example the functions implementing the VM operations are contained within the file VM.java.\n\n\nJava binding dependencies\n\n|||\n-------------|---------------|\n|Platform supported:|Linux and Windows|\n|Library:|  The language binding is generated as a Java Archive file xenserver-7.0.jar that is linked by Java programs.|\n|Dependencies:|\nxmlrpc-client-3.1.jar is needed for the xenserver.jar to be able to communicate with the xml-rpc server.\nws-commons-util-1.0.2.jar is needed to run the examples.\n|\n\n\nRunning the main file XenServer-SDK/XenServerJava/samples/RunTests.java will run a series of examples included in the same directory:\n\n\n\n\n\n\nAddNetwork: Adds a new internal network not attached to any NICs;\n\n\n\n\n\n\nSessionReuse: Demonstrates how a Session object can be shared between multiple Connections;\n\n\n\n\n\n\nAsyncVMCreate: Makes asynchronously a new VM from a built-in template, starts and stops it;\n\n\n\n\n\n\nVdiAndSrOps: Performs various SR and VDI tests, including creating a dummy SR;\n\n\n\n\n\n\nCreateVM: Creates a VM on the default SR with a network and DVD drive;\n\n\n\n\n\n\nDeprecatedMethod: Tests a warning is displayed when a deprecated API method is called;\n\n\n\n\n\n\nGetAllRecordsOfAllTypes: Retrieves all the records for all types of objects;\n\n\n\n\n\n\nSharedStorage: Creates a shared NFS SR;\n\n\n\n\n\n\nStartAllVMs: Connects to a host and tries to start each VM on it.\n\n\n\n\n\n\n4.2.4. PowerShell\n\n\nThe PowerShell bindings are contained within the directory XenServer-SDK/XenServerPowerShell. We provide the PowerShell module XenServerPSModule and source code exposing the XenServer API as Windows PowerShell cmdlets.\n\n\nPowerShell binding dependencies\n\n|||\n-------------|---------------|\n|Platform supported:|Windows with .NET Framework 4.5 and PowerShell v4.0|\n|Library:|XenServerPSModule|\n|Dependencies:|CookComputing.XMLRpcV2.dll is needed to be able to communicate with the xml-rpc server. We ship a patched 2.1.0.6 version and recommend that you use this one, though others may work.|\n\n\nThe following example scripts are included with the PowerShell bindings in the directory XenServer-SDK/XenServerPowerShell/samples:\n\n\n\n\n\n\nAutomatedTestCore.ps1: demonstrates how to log into a XenServer host, create a storage repository and a VM, and then perform various powercycle operations.\n\n\n\n\n\n\nHttpTest.ps1: demonstrates how to log into a XenServer host, create a VM, and then perform operations such as VM importing and exporting, patch upload, and retrieval of performance statistics.\n\n\n\n\n\n\n4.2.5. Python\n\n\nThe python bindings are contained within a single file: XenServer-SDK/XenServerPython/XenAPI.py.\n\n\nPython binding dependencies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlatform supported:\n\n\nLinux\n\n\n\n\n\n\nLibrary:\n\n\nXenAPI.py\n\n\n\n\n\n\nDependencies:\n\n\nNone\n\n\n\n\n\n\n\n\nThe SDK includes 7 python examples:\n\n\n\n\n\n\nfixpbds.py - reconfigures the settings used to access shared storage;\n\n\n\n\n\n\ninstall.py - installs a Debian VM, connects it to a network, starts it up and waits for it to report its IP address;\n\n\n\n\n\n\nlicense.py - uploads a fresh license to a XenServer Host;\n\n\n\n\n\n\npermute.py - selects a set of VMs and uses XenMotion to move them simultaneously between hosts;\n\n\n\n\n\n\npowercycle.py - selects a set of VMs and powercycles them;\n\n\n\n\n\n\nshell.py - a simple interactive shell for testing;\n\n\n\n\n\n\nvm_start_async.py - demonstrates how to invoke operations asynchronously;\n\n\n\n\n\n\nwatch-all-events.py - registers for all events and prints details when they occur.\n\n\n\n\n\n\n4.2.6. Command Line Interface (CLI)\n\n\nBesides using raw XML-RPC or one of the supplied language bindings, third-party software developers may integrate with XenServer Hosts by using the XE command line interface xe. The xe CLI is installed by default on XenServer hosts; a stand-alone remote CLI is also available for Linux. On Windows, the xe.exe CLI executable is installed along with XenCenter.\n\n\nCLI dependencies\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPlatform supported:\n\n\nLinux and Windows\n\n\n\n\n\n\nLibrary:\n\n\nNone\n\n\n\n\n\n\nBinary:\n\n\nxe (xe.exe on Windows)\n\n\n\n\n\n\nDependencies:\n\n\nNone\n\n\n\n\n\n\n\n\nThe CLI allows almost every API call to be directly invoked from a script or other program, silently taking care of the required session management. The XE CLI syntax and capabilities are described in detail in the XenServer Administrator's Guide. For additional resources and examples, visit the \nCitrix Knowledge Center\n.\n\n\n\n\nNote\n\nWhen running the CLI from a XenServer Host console, tab-completion of both command names and arguments is available.\n\n\n\n\n4.3. Complete application examples\n\n\nThis section describes two complete examples of real programs using the API.\n\n\n4.3.1. Simultaneously migrating VMs using XenMotion\n\n\nThis python example (contained in XenServer-SDK/XenServerPython/samples/permute.py) demonstrates how to use XenMotion to move VMs simultaneously between hosts in a Resource Pool. The example makes use of asynchronous API calls and shows how to wait for a set of tasks to complete.\n\n\nThe program begins with some standard boilerplate and imports the API bindings module\n\n\nimport\n \nsys\n,\n \ntime\n\n\nimport\n \nXenAPI\n\n\n\n\n\n\nNext the commandline arguments containing a server URL, username, password and a number of iterations are parsed. The username and password are used to establish a session which is passed to the function main, which is called multiple times in a loop. Note the use of try: finally: to make sure the program logs out of its session at the end.\n\n\nif __name__ == \n__main__\n:\n    if len(sys.argv) \n 5:\n        print \nUsage:\n\n        print sys.argv[0], \n \nurl\n \nusername\n \npassword\n \niterations\n\n        sys.exit(1)\n    url = sys.argv[1]\n    username = sys.argv[2]\n    password = sys.argv[3]\n    iterations = int(sys.argv[4])\n    # First acquire a valid session by logging in:\n    session = XenAPI.Session(url)\n    session.xenapi.login_with_password(username, password, \n2.3\n,\n                                       \nExample migration-demo v0.1\n)\n    try:\n        for i in range(iterations):\n            main(session, i)\n    finally:\n        session.xenapi.session.logout()\n\n\n\n\n\nThe main function examines each running VM in the system, taking care to filter out control domains (which are part of the system and not controllable by the user). A list of running VMs and their current hosts is constructed.\n\n\ndef main(session, iteration):\n    # Find a non-template VM object\n    all = session.xenapi.VM.get_all()\n    vms = []\n    hosts = []\n    for vm in all:\n        record = session.xenapi.VM.get_record(vm)\n        if not(record[\nis_a_template\n]) and \\\n           not(record[\nis_control_domain\n]) and \\\n           record[\npower_state\n] == \nRunning\n:\n            vms.append(vm)\n            hosts.append(record[\nresident_on\n])\n    print \n%d: Found %d suitable running VMs\n % (iteration, len(vms))\n\n\n\n\n\nNext the list of hosts is rotated:\n\n\n# use a rotation as a permutation\n    hosts = [hosts[-1]] + hosts[:(len(hosts)-1)]\n\n\n\n\n\nEach VM is then moved using XenMotion to the new host under this rotation (i.e. a VM running on host at position 2 in the list will be moved to the host at position 1 in the list etc.) In order to execute each of the movements in parallel, the asynchronous version of the VM.pool_migrate is used and a list of task references constructed. Note the live flag passed to the VM.pool_migrate; this causes the VMs to be moved while they are still running.\n\n\ntasks = []\n    for i in range(0, len(vms)):\n        vm = vms[i]\n        host = hosts[i]\n        task = session.xenapi.Async.VM.pool_migrate(vm, host, { \nlive\n: \ntrue\n })\n        tasks.append(task)\n\n\n\n\n\nThe list of tasks is then polled for completion:\n\n\nfinished = False\n    records = {}\n    while not(finished):\n        finished = True\n        for task in tasks:\n            record = session.xenapi.task.get_record(task)\n            records[task] = record\n            if record[\nstatus\n] == \npending\n:\n                finished = False\n        time.sleep(1)\n\n\n\n\n\nOnce all tasks have left the pending state (i.e. they have successfully completed, failed or been cancelled) the tasks are polled once more to see if they all succeeded:\n\n\nallok = True\n    for task in tasks:\n        record = records[task]\n        if record[\nstatus\n] \n \nsuccess\n:\n            allok = False\n\n\n\n\n\nIf any one of the tasks failed then details are printed, an exception is raised and the task objects left around for further inspection. If all tasks succeeded then the task objects are destroyed and the function returns.\n\n\nif\n \nnot\n(\nallok\n):\n\n        \nprint\n \nOne of the tasks didn\nt succeed at\n,\n \n\\\n\n            \ntime\n.strftime\n(\n%F:%HT%M:%SZ\n,\n \ntime\n.gmtime\n())\n\n        \nidx\n \n=\n \n0\n\n        \nfor\n \ntask\n \nin\n \ntasks\n:\n\n            \nrecord\n \n=\n \nrecords\n[\ntask\n]\n\n            \nvm_name\n \n=\n \nsession\n.xenapi.VM.get_name_label\n(\nvms\n[\nidx\n]\n)\n\n            \nhost_name\n \n=\n \nsession\n.xenapi.host.get_name_label\n(\nhosts\n[\nidx\n]\n)\n\n            \nprint\n \n%s : %12s %s -\n %s \n[\n \nstatus\n:\n \n%\ns\n;\n \nresult\n =\n \n%\ns\n;\n \nerror\n =\n \n%\ns\n \n]\n \n%\n \n\\\n\n                  \n(\nrecord\n[\nuuid\n]\n,\n \nrecord\n[\nname_label\n]\n,\n \nvm_name\n,\n \nhost_name\n,\n      \n\\\n\n                   \nrecord\n[\nstatus\n]\n,\n \nrecord\n[\nresult\n]\n,\n \nrepr\n(\nrecord\n[\nerror_info\n]\n))\n\n            \nidx\n \n=\n \nidx\n \n+\n \n1\n\n        \nraise\n \nTask failed\n\n    \nelse\n:\n\n        \nfor\n \ntask\n \nin\n \ntasks\n:\n\n            \nsession\n.xenapi.task.destroy\n(\ntask\n)\n\n\n\n\n\n\n4.3.2. Cloning a VM using the XE CLI\n\n\nThis example is a bash script which uses the XE CLI to clone a VM   taking care to shut it down first if it is powered on.\n\n\nThe example begins with some boilerplate which first checks if the environment variable XE has been set: if it has it assumes that it points to the full path of the CLI, else it is assumed that the XE CLI is on the current path. Next the script prompts the user for a server name, username and password:\n\n\n# Allow the path to the \nxe\n binary to be overridden by the XE environment variable\nif [ -z \n${\nXE\n}\n ]; then\n  XE=xe\nfi\n\nif [ ! -e \n${\nHOME\n}\n/.xe\n ]; then\n  read -p \nServer name: \n SERVER\n  read -p \nUsername: \n USERNAME\n  read -p \nPassword: \n PASSWORD\n  XE=\n${\nXE\n}\n -s \n${\nSERVER\n}\n -u \n${\nUSERNAME\n}\n -pw \n${\nPASSWORD\n}\n\nfi\n\n\n\n\n\nNext the script checks its commandline arguments. It requires exactly one: the UUID of the VM which is to be cloned:\n\n\n# Check if there\ns a VM by the uuid specified\n\n${\nXE\n}\n vm-list params=uuid | grep -q \n \n${\nvmuuid\n}\n$\n\nif [ $? -ne 0 ]; then\n        echo \nerror: no vm uuid \\\n${\nvmuuid\n}\n\\\n found\n\n        exit 2\nfi\n\n\n\n\n\nThe script then checks the power state of the VM and if it is running, it attempts a clean shutdown. The event system is used to wait for the VM to enter state \"Halted\".\n\n\n\n\nNote\n\nThe XE CLI supports a command-line argument --minimal which causes it to print its output without excess whitespace or formatting, ideal for use from scripts. If multiple values are returned they are comma-separated.\n\n\n\n\n# Check the power state of the vm\nname=$(\n${\nXE\n}\n vm-list uuid=\n${\nvmuuid\n}\n params=name-label --minimal)\nstate=$(\n${\nXE\n}\n vm-list uuid=\n${\nvmuuid\n}\n params=power-state --minimal)\nwasrunning=0\n\n# If the VM state is running, we shutdown the vm first\nif [ \n${\nstate\n}\n = \nrunning\n ]; then\n        \n${\nXE\n}\n vm-shutdown uuid=\n${\nvmuuid\n}\n\n        \n${\nXE\n}\n event-wait class=vm power-state=halted uuid=\n${\nvmuuid\n}\n\n        wasrunning=1\nfi\n\n\n\n\n\nThe VM is then cloned and the new VM has its name_label set to cloned_vm.\n\n\n# Clone the VM\nnewuuid=$(\n${\nXE\n}\n vm-clone uuid=\n${\nvmuuid\n}\n new-name-label=cloned_vm)\n\n\n\n\n\nFinally, if the original VM had been running and was shutdown, both it and the new VM are started.\n\n\n# If the VM state was running before cloning, we start it again\n# along with the new VM.\nif [ \n$wasrunning\n -eq 1 ]; then\n        \n${\nXE\n}\n vm-start uuid=\n${\nvmuuid\n}\n\n        \n${\nXE\n}\n vm-start uuid=\n${\nnewuuid\n}\n\nfi\n\n\n\n\n\nChapter 5. Using HTTP to interact with XenServer\n\n\nXenServer exposes an HTTP interface on each host, that can be used to perform various operations. This chapter describes the available mechanisms.\n\n\n5.1. VM Import and Export\n\n\nBecause the import and export of VMs can take some time to complete, an asynchronous HTTP interface to the import and export operations is provided. To perform an export using the XenServer API, construct an HTTP GET call providing a valid session ID, task ID and VM UUID, as shown in the following pseudo code:\n\n\ntask = Task.create()\nresult = HTTP.get(\n  server, 80, \n/export?session_id=\nsession_id\ntask_id=\ntask_id\nref=\nvm_uuid\n);\n\n\n\n\n\nFor the import operation, use an HTTP PUT call as demonstrated in the following pseudo code:\n\n\ntask =\n Task\n.\ncreate\n()\n\n\nresult =\n HTTP\n.\nput\n(\n\n  server\n,\n \n80\n,\n \n/import?session_id=\nsession_id\ntask_id=\ntask_id\nref=\nvm_uuid\n);\n\n\n\n\n\n\n5.2. Getting XenServer Performance Statistics\n\n\nXenServer records statistics about the performance of various aspects of your XenServer installation. The metrics are stored persistently for long term access and analysis of historical trends. Where storage is available to a VM, the statistics are written to disk when a VM is shut down. Statistics are stored in RRDs (Round Robin Databases), which are maintained for individual VMs (including the control domain) and the server. RRDs are resident on the server on which the VM is running, or the pool master when the VM is not running. The RRDs are also backed up every day.\n\n\n\n\nWarning\n\nIn earlier versions of the XenServer API, instantaneous performance metrics could be obtained using the VM_metrics, VM_guest_metrics, host_metrics methods and associated methods. These methods has been deprecated in favor of using the http handler described in this chapter to download the statistics from the RRDs on the VMs and servers. Note that by default the legacy metrics will return zeroes. To revert to periodic statistical polling as present in earlier versions of XenServer, set the other-config:rrd_update_interval=\n parameters on your host to one of the following values, and restart your host:\n\n\nnever\nThis is the default, meaning no periodic polling is performed.\n\n\n1 Polling is performed every 5 seconds.\n\n\n2 Polling is performed every minute.\n\n\nBy default, the older metrics APIs will not return any values, and so this key must be enabled to run monitoring clients which use the legacy monitoring protocol.\n\n\n\n\nStatistics are persisted for a maximum of one year, and are stored at different granularities. The average and most recent values are stored at intervals of:\n\n\n\n\n\n\n5 seconds for the past 10 minutes\n\n\n\n\n\n\none minute for the past 2 hours\n\n\n\n\n\n\none hour for the past week\n\n\n\n\n\n\none day for the past year\n\n\n\n\n\n\nRRDs are saved to disk as uncompressed XML. The size of each RRD when written to disk ranges from 200KiB to approximately 1.2MiB when the RRD stores the full year of statistics.\n\n\n\n\nWarning\n\nIf statistics cannot be written to disk, for example when a disk is full, statistics will be lost and the last saved version of the RRD will be used.\n\n\n\n\nStatistics can be downloaded over HTTP in XML format, for example using wget. See http://oss.oetiker.ch/rrdtool/doc/rrddump.en.html and http://oss.oetiker.ch/rrdtool/doc/rrdxport.en.html for information about the XML format. HTTP authentication can take the form of a username and password or a session token. Parameters are appended to the URL following a question mark (?) and separated by ampersands (\n).\n\n\nTo obtain an update of all VM statistics on a host, the URL would be of the form:\n\n\nhttp://\nusername\n:\npassword\n@\nhost\n/rrd_updates?start=\nsecondssinceepoch\n\nThis request returns data in an rrdtool xport style XML format, for every VM resident on the particular host that is being queried. To differentiate which column in the export is associated with which VM, the legend field is prefixed with the UUID of the VM.\n\n\nTo obtain host updates too, use the query parameter host=true:\n\n\nhttp://\nusername\n:\npassword\n@\nhost\n/rrd_updates?start=\nsecondssinceepoch\nhost=true\n\nThe step will decrease as the period decreases, which means that if you request statistics for a shorter time period you will get more detailed statistics.\n\n\nAdditional rrd_updates parameters\n\n\ncf=\nave|min|max\n   the data consolidation mode\n\ninterval=\ninterval\n the interval between values to be reported\n\n\n\n\nNote\n\nBy default only ave statistics are available. To obtain min and max statistics for a VM, run the following command:\n\nxe pool-param-set uuid=\npool_uuid\n other-config:create_min_max_in_new_VM_RRDs\n\nTo obtain all statistics for a host:\n\n\n\n\nhttp://\nusername:password@host\n/host_rrd\n\nTo obtain all statistics for a VM:\n\n\nhttp://\nusername:password@host\n/vm_rrd?uuid=\nvm_uuid\n\n\nChapter 6. XenServer API extensions\n\n\nThe XenAPI is a general and comprehensive interface to managing the life-cycles of Virtual Machines, and offers a lot of flexibility in the way that XenAPI providers may implement specific functionality (e.g. storage provisioning, or console handling). XenServer has several extensions which provide useful functionality used in our own XenCenter interface. The workings of these mechanisms are described in this chapter.\n\n\nExtensions to the XenAPI are often provided by specifying other-config map keys to various objects. The use of this parameter indicates that the functionality is supported for that particular release of XenServer, but not as a long-term feature. We are constantly evaluating promoting functionality into the API, but this requires the nature of the interface to be well-understood. Developer feedback as to how you are using some of these extensions is always welcome to help us make these decisions.\n\n\n6.1. VM console forwarding\n\n\nMost XenAPI graphical interfaces will want to gain access to the VM consoles, in order to render them to the user as if they were physical machines. There are several types of consoles available, depending on the type of guest or if the physical host console is being accessed:\n\n\nConsole access\n\n\n\n\n\n\n\n\nOperating System\n\n\nText\n\n\nGraphical\n\n\nOptimized graphical\n\n\n\n\n\n\n\n\n\n\nWindows\n\n\nNo\n\n\nVNC, using an API call\n\n\nRDP, directly from guest\n\n\n\n\n\n\nLinux\n\n\nYes, through VNC and an API call\n\n\nNo\n\n\nVNC, directly from guest\n\n\n\n\n\n\nPhysical Host\n\n\nYes, through VNC and an API call\n\n\nNo\n\n\nNo\n\n\n\n\n\n\n\n\nHardware-assisted VMs, such as Windows, directly provide a graphical console over VNC. There is no text-based console, and guest networking is not necessary to use the graphical console. Once guest networking has been established, it is more efficient to setup Remote Desktop Access and use an RDP client to connect directly (this must be done outside of the XenAPI).\n\n\nParavirtual VMs, such as Linux guests, provide a native text console directly. XenServer provides a utility (called vncterm) to convert this text-based console into a graphical VNC representation. Guest networking is not necessary for this console to function. As with Windows above, Linux distributions often configure VNC within the guest, and directly connect to it over a guest network interface.\n\n\nThe physical host console is only available as a vt100 console, which is exposed through the XenAPI as a VNC console by using vncterm in the control domain.\n\n\nRFB (Remote Framebuffer) is the protocol which underlies VNC, specified in The RFB Protocol. Third-party developers are expected to provide their own VNC viewers, and many freely available implementations can be adapted for this purpose. RFB 3.3 is the minimum version which viewers must support.\n\n\n6.1.1. Retrieving VNC consoles using the API\n\n\nVNC consoles are retrieved using a special URL passed through to the host agent. The sequence of API calls is as follows:\n\n\n\n\n\n\nClient to Master/443: XML-RPC: Session.login_with_password().\n\n\n\n\n\n\nMaster/443 to Client: Returns a session reference to be used with subsequent calls.\n\n\n\n\n\n\nClient to Master/443: XML-RPC: VM.get_by_name_label().\n\n\n\n\n\n\nMaster/443 to Client: Returns a reference to a particular VM (or the \"control domain\" if you want to retrieve the physical host console).\n\n\n\n\n\n\nClient to Master/443: XML-RPC: VM.get_consoles().\n\n\n\n\n\n\nMaster/443 to Client: Returns a list of console objects associated with the VM.\n\n\n\n\n\n\nClient to Master/443: XML-RPC: VM.get_location().\n\n\n\n\n\n\nReturns a URI describing where the requested console is located. The URIs are of the form: https://192.168.0.1/console?ref=OpaqueRef:c038533a-af99-a0ff-9095-c1159f2dc6a0.\n\n\n\n\n\n\nClient to 192.168.0.1: HTTP CONNECT \"/console?ref=(...)\"\n\n\n\n\n\n\nThe final HTTP CONNECT is slightly non-standard since the HTTP/1.1 RFC specifies that it should only be a host and a port, rather than a URL. Once the HTTP connect is complete, the connection can subsequently directly be used as a VNC server without any further HTTP protocol action.\n\n\nThis scheme requires direct access from the client to the control domain's IP, and will not work correctly if there are Network Address Translation (NAT) devices blocking such connectivity. You can use the CLI to retrieve the console URI from the client and perform a connectivity check.\n\n\nTo retrieve a console URI using the CLI\n\n\n\n\n\n\nRetrieve the VM UUID by running:\n\n\nxe vm-list params=uuid --minimal name-label=name\n\n\n\n\n\n\nRetrieve the console information:\n\n\n\n\n\n\nxe console-list vm-uuid=uuid\nuuid ( RO): 714f388b-31ed-67cb-617b-0276e35155ef\nvm-uuid ( RO): 8acb7723-a5f0-5fc5-cd53-9f1e3a7d3069\nvm-name-label ( RO): etch\nprotocol ( RO): RFB\nlocation ( RO): https://192.168.0.1/console?ref=(...)\nUse command-line utilities like ping to test connectivity to the IP address provided in the location field.\n\n\n\n\n\n6.1.2. Disabling VNC forwarding for Linux VM\n\n\nWhen creating and destroying Linux VMs, the host agent automatically manages the vncterm processes which convert the text console into VNC. Advanced users who wish to directly access the text console can disable VNC forwarding for that VM. The text console can then only be accessed directly from the control domain directly, and graphical interfaces such as XenCenter will not be able to render a console for that VM.\n\n\nDisabling a Linux VNC console using the CLI\n\n\n\n\n\n\nBefore starting the guest, set the following parameter on the VM record:\n\n\nxe vm-param-set uuid=uuid other-config:disable_pv_vnc=1\n\n2. Start the VM.\n\n\n\n\n\n\nUse the CLI to retrieve the underlying domain ID of the VM with:\n    `xe vm-list params=dom-id uuid=\n --minimal\n\n\n\n\nOn the host console, connect to the text console directly by:\n/usr/lib/xen/bin/xenconsole \n`\nThis configuration is an advanced procedure, and we do not recommend that the text console is directly used for heavy I/O operations. Instead, connect to the guest over SSH or some other network-based connection mechanism.\n\n\n\n\n6.2. Paravirtual Linux installation\n\n\nThe installation of paravirtual Linux guests is complicated by the fact that a Xen-aware kernel must be booted, rather than simply installing the guest using hardware-assistance. This does have the benefit of providing near-native installation speed due to the lack of emulation overhead. XenServer supports the installation of several different Linux distributions, and abstracts this process as much as possible.\n\n\nTo this end, a special bootloader known as eliloader is present in the control domain which reads various other-config keys in the VM record at start time and performs distribution-specific installation behavior.\n\n\n\n\n\n\ninstall-repository - Required. Path to a repository; 'http', 'https', 'ftp', or 'nfs'. Should be specified as would be used by the target installer, but not including prefixes, e.g. method=.\n\n\n\n\n\n\ninstall-vnc - Default: false. Use VNC where available during the installation.\n\n\n\n\n\n\ninstall-vncpasswd - Default: empty. The VNC password to use, when providing one is possible using the command-line of the target distribution.\n\n\n\n\n\n\ninstall-round - Default: 1. The current bootloader round. Not to be edited by the user (see below)\n\n\n\n\n\n\n6.2.1. Red Hat Enterprise Linux 4.1/4.4\n\n\neliloader is used for two rounds of booting. In the first round, it returns the installer initrd and kernel from /opt/xensource/packages/files/guest-installer. Then, on the second boot, it removes the additional updates disk from the VM, switches the bootloader to pygrub, and then begins a normal boot.\n\n\nThis sequence is required since Red Hat does not provide a Xen kernel for these distributions, and so the XenServer custom kernels for those distributions are used instead.\n\n\n6.2.2. Red Hat Enterprise Linux 4.5/5.0\n\n\nSimilar to the RHEL4.4 installation, except that the kernel and ramdisk are downloaded directly form the network repository that was specified by the user, and switch the bootloader to pygrub immediately. Note that pygrub is not executed immediately, and so will only be parsed on the next boot.\n\n\nThe network retrieval enables users to install the upstream Red Hat vendor kernel directly from their network repository. An updated XenServer kernel is also provided on the xs-tools.iso built-in ISO image which fixes various Xen-related bugs.\n\n\n6.2.3. SUSE Enterprise Linux 10 SP1\n\n\nThis requires a two-round boot process. The first round downloads the kernel and ramdisk from the network repository and boots them. The second round then inspects the disks to find the installed kernel and ramdisk, and sets the PV-bootloader-args to reflect these paths within the guest filesystem. This process emulates the domUloader which SUSE use as an alternative to pygrub. Finally, the bootloader is set to pygrub and is executed to begin a normal boot.\n\n\nThe SLES 10 installation method means that the path for the kernel and ramdisk is stored in the VM record rather than in the guest menu.lst, but this is the only way it would ever work since the YAST package manager doesn't write a valid menu.lst.\n\n\n6.2.4. CentOS 4.5 / 5.0\n\n\nThe CentOS installation mechanism is similar to that of the Red Hat installation notes above, save that some MD5 checksums are different which eliloader recognizes.\n\n\n6.3. Adding Xenstore entries to VMs\n\n\nDevelopers may wish to install guest agents into VMs which take special action based on the type of the VM. In order to communicate this information into the guest, a special Xenstore name-space known as vm-data is available which is populated at VM creation time. It is populated from the xenstore-data map in the VM record.\n\n\nTo populate a Xenstore node foo in a VM\n\n\n\n\n\n\nSet the xenstore-data parameter in the VM record:\n\n\nxe vm-param-set uuid=\nvm_uuid\n xenstore-data:vm-data/foo=bar\u00a1\u00a4\n\n\n\n\n\n\nStart the VM.\n\n\n\n\n\n\nIf it is a Linux-based VM, install the XenServer Tools and use the xenstore-read to verify that the node exists in Xenstore.\n\n\n\n\n\n\n\n\nNote\n\nOnly prefixes beginning with vm-data are permitted, and anything not in this name-space will be silently ignored when starting the VM.\n\n\n\n\n6.4. Security enhancements\n\n\nThe control domain in XenServer 7.0 and above has various security enhancements in order to harden it against attack from malicious guests. Developers should never notice any loss of correct functionality as a result of these changes, but they are documented here as variations of behavior from other distributions.\n\n\n\n\n\n\nThe socket interface, xenstored, access using libxenstore. Interfaces are restricted by xs_restrict().\n\n\n\n\n\n\nThe device /dev/xen/evtchn, which is accessed by calling xs_evtchn_open() in libxenctrl. A handle can be restricted using xs_evtchn_restrict().\n\n\n\n\n\n\nThe device /proc/xen/privcmd, accessed through xs_interface_open() in libxenctrl. A handle is restricted using xc_interface_restrict(). Some privileged commands are naturally hard to restrict (e.g. the ability to make arbitrary hypercalls), and these are simply prohibited on restricted handles.\n\n\n\n\n\n\nA restricted handle cannot later be granted more privilege, and so the interface must be closed and re-opened. Security is only gained if the process cannot subsequently open more handles.\n\n\n\n\n\n\nThe control domain privileged user-space interfaces can now be restricted to only work for certain domains. There are three interfaces affected by this change:\n\n\n\n\n\n\nThe qemu device emulation processes and vncterm terminal emulation processes run as a non-root user ID and are restricted into an empty directory. They uses the restriction API above to drop privileges where possible.\n\n\n\n\n\n\nAccess to xenstore is rate-limited to prevent malicious guests from causing a denial of service on the control domain. This is implemented as a token bucket with a restricted fill-rate, where most operations take one token and opening a transaction takes 20. The limits are set high enough that they should never be hit when running even a large number of concurrent guests under loaded operation.\n\n\n\n\n\n\nThe VNC guest consoles are bound only to the localhost interface, so that they are not exposed externally even if the control domain packet filter is disabled by user intervention.\n\n\n\n\n\n\n6.5. Advanced settings for network interfaces\n\n\nVirtual and physical network interfaces have some advanced settings that can be configured using the other-config map parameter. There is a set of custom ethtool settings and some miscellaneous settings.\n\n\n6.5.1. ethtool settings\n\n\nDevelopers might wish to configure custom ethtool settings for physical and virtual network interfaces. This is accomplished with ethtool-\n keys in the other-config map parameter.\n\n\n\n\n\n\n\n\nKey\n\n\nDescription\n\n\nValid settings\n\n\n\n\n\n\n\n\n\n\nethtool-rx\n\n\nSpecify if RX checksumming is enabled\n\n\non or true to enable the setting, off or false to disable it\n\n\n\n\n\n\nethtool-tx\n\n\nSpecify if TX checksumming is enabled\n\n\non or true to enable the setting, off or false to disable it\n\n\n\n\n\n\nethtool-sg\n\n\nSpecify if scatter-gather is enabled\n\n\non or true to enable the setting, off or false to disable it\n\n\n\n\n\n\nethtool-tso\n\n\nSpecify if tcp segmentation offload is enabled\n\n\non or true to enable the setting, off or false to disable it\n\n\n\n\n\n\nethtool-ufo\n\n\nSpecify if UDP fragmentation offload is enabled\n\n\non or true to enable the setting, off or false to disable it\n\n\n\n\n\n\nethtool-gso\n\n\nSpecify if generic segmentation offload is enabled\n\n\non or true to enable the setting, off or false to disable it\n\n\n\n\n\n\nethtool-autoneg\n\n\nSpecify if autonegotiation is enabled\n\n\non or true to enable the setting, off or false to disable it\n\n\n\n\n\n\nethtool-speed\n\n\nSet the device speed in Mb/s\n\n\n10, 100, or 1000\n\n\n\n\n\n\nethtool-duplex\n\n\nSet full or half duplex mode\n\n\nhalf or full\n\n\n\n\n\n\n\n\nFor example, to enable TX checksumming on a virtual NIC using the xe CLI:\n\nxe vif-param-set uuid=\nVIF UUID\n other-config:ethtool-tx=\"on\"\n\n\nor:\n\n\nxe vif-param-set uuid=\nVIF UUID\n other-config:ethtool-tx=\"true\"\n\nTo set the duplex setting on a physical NIC to half duplex using the xe CLI:\n\n\nxe vif-param-set uuid=\nVIF UUID\n other-config:ethtool-duplex=\"half\"\n\n\n6.5.2. Miscellaneous settings\n\n\nYou can also set a promiscuous mode on a VIF or PIF by setting the promiscuous key to on. For example, to enable promiscuous mode on a physical NIC using the xe CLI:\n\n\nxe pif-param-set uuid=\nPIF UUID\n other-config:promiscuous=\"on\"\n\nor:\n\n\nxe pif-param-set uuid=\nPIF UUID\n other-config:promiscuous=\"true\"\n\nThe VIF and PIF objects have a MTU parameter that is read-only and provide the current setting of the maximum transmission unit for the interface. You can override the default maximum transmission unit of a physical or virtual NIC with the mtu key in the other-config map parameter. For example, to reset the MTU on a virtual NIC to use jumbo frames using the xe CLI:\n\n\nxe vif-param-set uuid=\nVIF UUID\n other-config:mtu=9000\n\nNote that changing the MTU of underlying interfaces is an advanced and experimental feature, and may lead to unexpected side-effects if you have varying MTUs across NICs in a single resource pool.\n\n\n6.6. Internationalization for SR names\n\n\nThe SRs created at install time now have an other_config key indicating how their names may be internationalized.\n\n\nother_config[\"i18n-key\"] may be one of\n\n\n\n\n\n\nlocal-hotplug-cd\n\n\n\n\n\n\nlocal-hotplug-disk\n\n\n\n\n\n\nlocal-storage\n\n\n\n\n\n\nxenserver-tools\n\n\n\n\n\n\nAdditionally, other_config[\"i18n-original-value-\n\"] gives the value of that field when the SR was created. If XenCenter sees a record where SR.name_label equals other_config[\"i18n-original-value-name_label\"] (that is, the record has not changed since it was created during XenServer installation), then internationalization will be applied. In other words, XenCenter will disregard the current contents of that field, and instead use a value appropriate to the user's own language.\n\n\nIf you change SR.name_label for your own purpose, then it no longer is the same as other_config[\"i18n-original-value-name_label\"]. Therefore, XenCenter does not apply internationalization, and instead preserves your given name.\n\n\n6.7. Hiding objects from XenCenter\n\n\nNetworks, PIFs, and VMs can be hidden from XenCenter by adding the key HideFromXenCenter=true to the other_config parameter for the object. This capability is intended for ISVs who know what they are doing, not general use by everyday users. For example, you might want to hide certain VMs because they are cloned VMs that shouldn't be used directly by general users in your environment.\n\n\nIn XenCenter, hidden Networks, PIFs, and VMs can be made visible, using the \nView\n menu.\n\n\nChapter 7. XenCenter API Extensions\n\n\nThe following section details the assumptions and API extensions that we have made, over and above the documented API. Extensions are encoded as particular key-value pairs in dictionaries such as VM.other_config.\n\n\n7.1. Pool\n\n\n\n\n\n\n\n\nKey\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\npool.name_label\n\n\nAn empty name_label indicates that the pool should be hidden on the tree view.\n\n\n\n\n\n\npool.rolling_upgrade_in_progress\n\n\nPresent if the pool is in the middle of a rolling upgrade.\n\n\n\n\n\n\n\n\n7.2. Host\n\n\n\n\n\n\n\n\nKey\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\nhost.other_config[\"iscsi_iqn\"]\n\n\nThe host's iSCSI IQN.\n\n\n\n\n\n\nhost.license_params[\"expiry\"]\n\n\nThe expiry date of the host's license, in ISO 8601, UTC.\n\n\n\n\n\n\nhost.license_params[\"sku_type\"]\n\n\nThe host license type i.e. Server or Enterprise.\n\n\n\n\n\n\nhost.license_params[\"restrict_pooling\"]\n\n\nReturns true if pooling is restricted by the host.\n\n\n\n\n\n\nhost.license_params[\"restrict_connection\"]\n\n\nThe number of connections that can be made from XenCenter is restricted.\n\n\n\n\n\n\nhost.license_params[\"restrict_qos\"]\n\n\nReturns true if Quality of Service settings are enabled on the host.\n\n\n\n\n\n\nhost.license_params[\"restrict_vlan\"]\n\n\nReturns true if creation of virtual networks is restricted on the host.\n\n\n\n\n\n\nhost.license_params[\"restrict_pool_attached_storage\"]\n\n\nReturns true if the creation of shared storage is restricted on this host.\n\n\n\n\n\n\nhost.software_version[\"product_version\"]\n\n\nReturns the host's product version.\n\n\n\n\n\n\nhost.software_version[\"build_number\"]\n\n\nReturns the host's build number.\n\n\n\n\n\n\nhost.software_version[\"xapi\"]\n\n\nReturns the host's api revision number.\n\n\n\n\n\n\nhost.software_version[\"package-linux\"]\n\n\nReturns \"installed\" if the Linux pack has been installed.\n\n\n\n\n\n\nhost.software_version[\"oem_build_number\"]\n\n\nIf the host is the OEM version, return its revision number.\n\n\n\n\n\n\nhost.logging[\"syslog_destination\"]\n\n\nGets or sets the destination for the XenServer system logger (null for local logging).\n\n\n\n\n\n\nhost.logging[\"multipathing\"]\n\n\n\"true\" if storage multipathing is enabled on this host.\n\n\n\n\n\n\nhost.logging[\"boot_time\"]\n\n\nA floating point Unix time giving the time that the host booted.\n\n\n\n\n\n\nhost.logging[\"agent_start_time\"]\n\n\nA floating point Unix time giving the time that the control domain management daemon started.\n\n\n\n\n\n\n\n\n7.3. VM\n\n\n\n\n\n\n\n\nKey\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\nVM.other_config[\"default_template\"]\n\n\nThis template is one that was installed by Citrix. This is used to selectively hide these in the tree view, to use a different icon for them, and to disallow deletion.\n\n\n\n\n\n\nVM.other_config[\"xensource_internal\"]\n\n\nThis template is special, such as the P2V server template. These are completely hidden by the UI.\n\n\n\n\n\n\nVM.other_config[\"install_distro\"] == \"rhlike\"\n\n\nThis template is for RHEL 4.5, RHEL 5, or CentOS equivalents. This is used to prompt for the Install Repository during install, including support for install from ISO / CD on Miami, and to modify NFS URLs to suit these installers.\n\n\n\n\n\n\nVM.other_config[\"install_distro\"] in { \"rhel41\"\n\n\n\"rhel44\" }\n\n\n\n\n\n\nVM.other_config[\"install_distro\"] == \"sleslike\"\n\n\nThis template is for SLES 10 and SLES 9. This is used to prompt for the Install Repository during install, like the EL5 ones, but in this case the NFS URLs are not modified. ISO support is available for SLES 10 on XenServer 7.0. Use install-methods to distinguish between SLES 9 and SLES 10 on that platform.\n\n\n\n\n\n\nVM.other_config[\"install-repository\"] == \"cdrom\"\n\n\nRequests an install from a repository in the VM's attached CD drive, rather than a URL.\n\n\n\n\n\n\nVM.other_config[\"auto_poweron\"]\n\n\nGets or sets whether the VM starts when the server boots, \"true\" or \"false\".\n\n\n\n\n\n\nVM.other_config[\"ignore_excessive_vcpus\"]\n\n\nGets or sets to ignore XenCenter's warning if a VM has more VCPUs than its host has physical CPUs, true to ignore.\n\n\n\n\n\n\nVM.other_config[\"HideFromXenCenter\"]\n\n\nGets or sets whether XenCenter will show the VM in the treeview, \"true\" to hide.\n\n\n\n\n\n\nVM.other_config[\"import_task\"]\n\n\nGets the import task that created this VM.\n\n\n\n\n\n\nVM.HVM_boot_params[\"order\"]\n\n\nGets or sets the VM's boot order on HVM VM's only, e.g. \"CDN\" will boot in the following order - First boot disk, CD drive, Network.\n\n\n\n\n\n\nVM.VCPU_params[\"weight\"]\n\n\nGets or sets the IONice value for the VM's VCPUs, ranges from 1 to 65536, 65536 being the highest.\n\n\n\n\n\n\nVM.pool_migrate(..., options['live'])\n\n\ntrue indicates live migration. XenCenter always uses this.\n\n\n\n\n\n\nVM.other_config[\"install-methods\"]\n\n\nA comma-separated list of install methods available for this template. May include \"cdrom\", \"nfs\", \"http\" or \"ftp\".\n\n\n\n\n\n\nVM.other_config[\"last_shutdown_time\"]\n\n\nThe time that this VM was last shut down or rebooted, formatted as a UTC ISO8601 datetime.\n\n\n\n\n\n\nVM.other_config[\"p2v_source_machine\"]\n\n\nThe source machine, if this VM was imported by a P2V process.\n\n\n\n\n\n\nVM.other_config[\"p2v_import_date\"]\n\n\nThe date the VM was imported, if it was imported by a P2V process. Formatted as a UTC ISO8601 datetime.\n\n\n\n\n\n\n\n\n7.4.  SR\n\n\n\n\n\n\n\n\nKey\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\nSR.other_config[\"auto-scan\"]\n\n\nThe SR will be automatically scanned for changes. Set on all SRs created by XenCenter.\n\n\n\n\n\n\nSR.sm_config[\"type\"]\n\n\nSet as type cd for SRs which are physical CD drives.\n\n\n\n\n\n\n\n\n7.5. VDI\n\n\n\n\n\n\n\n\nKey\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\nVDI.type\n\n\nuser instead of system is used to mean \"do or do not allow deletion of the VDI through the GUI, if this disk is attached to a VM\". The intention here is to prevent you from corrupting a VM (you should uninstall it instead). suspend and crashdump record suspend and core dumps respectively. ephemeral is currently unused.\n\n\n\n\n\n\nVDI.managed\n\n\nAll unmanaged VDIs are completely hidden in the UI. These are branch points in VHD chains, or unused LUN-per-VDI disks.\n\n\n\n\n\n\nVDI.sm_config[\"vmhint\"]\n\n\nThe UUID of the VM that this VDI supports. This is set when VDIs are created through the user interface, to improve performance for certain storage backends.\n\n\n\n\n\n\n\n\n7.6. VBD\n\n\n\n\n\n\n\n\nKey\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\nVBD.other_config[\"is_owner\"]\n\n\nIf set, then this disk may be deleted when the VM is uninstalled.\n\n\n\n\n\n\nVBD.other_config[\"class\"]\n\n\nSet to an integer, corresponding to the Best Effort setting of ionice.\n\n\n\n\n\n\n\n\n7.7. Network\n\n\n\n\n\n\n\n\nKey\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\nnetwork.other_config[\"automatic\"]\n\n\nThe New VM wizard will create a VIF connected to this network by default, if this key has any value other than false.\n\n\n\n\n\n\nnetwork.other_config[\"import_task\"]\n\n\nGets the import task that created this network.\n\n\n\n\n\n\n\n\n7.8. VM_guest_metrics\n\n\n\n\n\n\n\n\nKey\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\nPV_drivers_version[\"major\"]\n\n\nGets the major version of the VM's PV drivers' version.\n\n\n\n\n\n\nPV_drivers_version[\"minor\"]\n\n\nGets the minor version of the VM's PV drivers' version.\n\n\n\n\n\n\nPV_drivers_version[\"micro\"]\n\n\nGets the micro (build number) of the VM's PV drivers' version.\n\n\n\n\n\n\n\n\n7.9. Task\n\n\n\n\n\n\n\n\nKey\n\n\nSemantics\n\n\n\n\n\n\n\n\n\n\ntask.other_config[\"object_creation\"] == \"complete\"\n\n\nFor the task associated with a VM import, this flag will be set when all the objects (VMs, networks) have been created. This is useful in the import VM wizard for us to then go and re-map all the networks that need it.", 
            "title": "Index"
        }, 
        {
            "location": "/#citrix-xenserver-70-software-development-kit", 
            "text": "", 
            "title": "Citrix XenServer 7.0 Software Development Kit"
        }, 
        {
            "location": "/#chapter-1-introduction", 
            "text": "Welcome to the developer's guide for XenServer. Here you will find the information you need in order to understand and use the Software Development Kit (SDK) that XenServer provides. This information will provide you with some of the architectural background and thinking that underpins the APIs, the tools that have been provided, and how to quickly get off the ground.", 
            "title": "Chapter 1. Introduction"
        }, 
        {
            "location": "/#chapter-2-getting-started", 
            "text": "XenServer includes a XML-RPC based API providing programmatic access to the extensive set of XenServer management features and tools. The XenServer API can be called from a remote system as well as local to the XenServer host. Although it is possible to write applications which use the XenServer Management API directly through raw XML-RPC calls, the task of developing third-party applications is greatly simplified through the use of a language binding which exposes the individual API calls as first-class functions in the target language. The XenServer SDK provides language bindings and example code for the C, C#, Java, Python and PowerShell programming languages.", 
            "title": "Chapter 2. Getting Started"
        }, 
        {
            "location": "/#21-system-requirements-and-preparation", 
            "text": "The first step towards working with the SDK is to install XenServer. A free version, Citrix XenServer, is available for download at http://www.citrix.com/downloads/xenserver/. Please refer to the XenServer Installation Guide for detailed instructions on how to set up your development host. When the installation is complete, please note the host IP address and the host password.", 
            "title": "2.1. System Requirements and Preparation"
        }, 
        {
            "location": "/#22-downloading", 
            "text": "The SDK is packaged as a ZIP file and is available as a free download from http://www.citrix.com/downloads/xenserver/ .", 
            "title": "2.2. Downloading"
        }, 
        {
            "location": "/#23-content-map", 
            "text": "The extracted contents of the SDK ZIP file are in the XenServer-SDK directory. The following is an overview of its structure. Where necessary, subdirectories have their own individual README files. It should be noted that the examples provided are not the same across all the language bindings, so, if you intend to use one binding, it is advisable to also browse the sample code available in the others.\n- XenServer-SDK.   - libxenserver. The XenServer SDK for C.\n\n     - bin. libxenserver binaries.\n\n    - src. libxenserver source code and examples and a Makefile to build them.\n\n- XenServer.NET. The XenServer SDK for C#.NET.\n\n    - bin. XenServer.NET binaries.\n\n    - samples. XenServer.NET examples.\n\n    - src. XenServer.NET source code.\n\n- XenServerJava. The XenServer SDK for Java\n\n    - bin. Java binaries.\n\n    - javadoc. Java documentation.\n\n    - samples. Java examples.\n\n    - src. Java source code and a Makefile to build the code and the examples.\n\n- XenServerPowerShell. The XenServer SDK for PowerShell.\n\n    - XenServerPSModule.  The XenServer PowerShell module.\n\n    - samples. PowerShell example scripts.\n\n    - src. XenServer PowerShell source code.\n\n- XenServerPython. This directory contains the XenServer Python module XenAPI.py. The library provision.py is used by the examples.\n    - samples. XenServer API examples using Python.", 
            "title": "2.3. Content Map"
        }, 
        {
            "location": "/#chapter-3-overview-of-the-xenserver-api", 
            "text": "This chapter introduces the XenServer API (subsequently referred to as the \"API\") and its associated object model. The API has the following key features:    Management of all aspects of the XenServer Host. The API allows you to manage VMs, storage, networking, host configuration and pools. Performance and status metrics can also be queried from the API.    Persistent Object Model. The results of all side-effecting operations (e.g. object creation, deletion and parameter modifications) are persisted in a server-side database that is managed by the XenServer installation.    An event mechanism. Through the API, clients can register to be notified when persistent (server-side) objects are modified. This enables applications to keep track of datamodel modifications performed by concurrently executing clients.    Synchronous and asynchronous invocation. All API calls can be invoked synchronously (that is, block until completion); any API call that may be long-running can also be invoked asynchronously. Asynchronous calls return immediately with a reference to a task object. This task object can be queried (through the API) for progress and status information. When an asynchronously invoked operation completes, the result (or error code) is available from the task object.    Remotable and Cross-Platform. The client issuing the API calls does not have to be resident on the host being managed; nor does it have to be connected to the host over ssh in order to execute the API. API calls make use of the XML-RPC protocol to transmit requests and responses over the network.    Secure and Authenticated Access. The XML-RPC API server executing on the host accepts secure socket connections. This allows a client to execute the APIs over the https protocol. Further, all the API calls execute in the context of a login session generated through username and password validation at the server. This provides secure and authenticated access to the XenServer installation.", 
            "title": "Chapter 3. Overview of the XenServer API"
        }, 
        {
            "location": "/#31-getting-started-with-the-api", 
            "text": "We will start our tour of the API by describing the calls required to create a new VM on a XenServer installation, and take it through a start/suspend/resume/stop cycle. This is done without reference to code in any specific language; at this stage we just describe the informal sequence of RPC invocations that accomplish our \"install and start\" task.   Note \nWe recommend strongly against using the VM.create call, which may be removed or changed in a future version of the API. Read on to learn other ways to make a new VM.", 
            "title": "3.1. Getting Started with the API"
        }, 
        {
            "location": "/#311-authentication-acquiring-a-session-reference", 
            "text": "The first step is to call Session.login_with_password( username ,  password ,  client_API_version ,  originator ). The API is session based, so before you can make other calls you will need to authenticate with the server. Assuming the username and password are authenticated correctly, the result of this call is a session reference. Subsequent API calls take the session reference as a parameter. In this way we ensure that only API users who are suitably authorized can perform operations on a XenServer installation. You can continue to use the same session for any number of API calls. When you have finished the session, Citrix recommends that you call  Session.logout(session)  to clean up: see later.", 
            "title": "3.1.1. Authentication: acquiring a session reference"
        }, 
        {
            "location": "/#312-acquiring-a-list-of-templates-to-base-a-new-vm-installation-on", 
            "text": "The next step is to query the list of \"templates\" on the host. Templates are specially-marked VM objects that specify suitable default parameters for a variety of supported guest types. (If you want to see a quick enumeration of the templates on a XenServer installation for yourself then you can execute the xe template-list CLI command.) To get a list of templates from the API, we need to find the VM objects on the server that have their is_a_template field set to true. One way to do this by calling VM.get_all_records(session) where the session parameter is the reference we acquired from our Session.login_with_password call earlier. This call queries the server, returning a snapshot (taken at the time of the call) containing all the VM object references and their field values.  (Remember that at this stage we are not concerned about the particular mechanisms by which the returned object references and field values can be manipulated in any particular client language: that detail is dealt with by our language-specific API bindings and described concretely in the following chapter. For now it suffices just to assume the existence of an abstract mechanism for reading and manipulating objects and field values returned by API calls.)  Now that we have a snapshot of all the VM objects' field values in the memory of our client application we can simply iterate through them and find the ones that have their \"is_a_template\" set to true. At this stage let's assume that our example application further iterates through the template objects and remembers the reference corresponding to the one that has its \"name_label\" set to \"Debian Etch 4.0\" (one of the default Linux templates supplied with XenServer).", 
            "title": "3.1.2. Acquiring a list of templates to base a new VM installation on"
        }, 
        {
            "location": "/#313-installing-the-vm-based-on-a-template", 
            "text": "Continuing through our example, we must now install a new VM based on the template we selected. The installation process requires 2 API calls:    First we must now invoke the API call VM.clone(session, t_ref, \"my first VM\"). This tells the server to clone the VM object referenced by t_ref in order to make a new VM object. The return value of this call is the VM reference corresponding to the newly-created VM. Let's call this new_vm_ref.    At this stage the object referred to by new_vm_ref is still a template (just like the VM object referred to by t_ref, from which it was cloned). To make new_vm_ref into a VM object we need to call VM.provision(session, new_vm_ref). When this call returns the new_vm_ref object will have had its is_a_template field set to false, indicating that new_vm_ref now refers to a regular VM ready for starting.     Note \nThe provision operation may take a few minutes, as it is as during this call that the template's disk images are created. In the case of the Debian template, the newly created disks are also at this stage populated with a Debian root filesystem.", 
            "title": "3.1.3. Installing the VM based on a template"
        }, 
        {
            "location": "/#314-taking-the-vm-through-a-startsuspendresumestop-cycle", 
            "text": "Now we have an object reference representing our newly-installed VM, it is trivial to take it through a few lifecycle operations:    To start our VM we can just call VM.start(session, new_vm_ref)    After it's running, we can suspend it by calling VM.suspend(session, new_vm_ref),    and then resume it by calling VM.resume(session, new_vm_ref).    We can call VM.shutdown(session, new_vm_ref) to shutdown the VM cleanly.", 
            "title": "3.1.4. Taking the VM through a start/suspend/resume/stop cycle"
        }, 
        {
            "location": "/#315-logging-out", 
            "text": "Once an application is finished interacting with a XenServer Host it is good practice to call Session.logout(session). This invalidates the session reference (so it cannot be used in subsequent API calls) and simultaneously deallocates server-side memory used to store the session object.  Although inactive sessions will eventually timeout, the server has a hardcoded limit of 500 concurrent sessions for each username or originator. Once this limit has been reached fresh logins will evict the session objects that have been used least recently, causing their associated session references to become invalid. For successful interoperability with other applications, concurrently accessing the server, the best policy is:    Choose a string that identifies your application and its version.    Create a single session at start-of-day, using that identifying string for the originator parameter to Session.login_with_password.    Use this session throughout the application (note that sessions can be used across multiple separate client-server network connections) and then explicitly logout when possible.    If a poorly written client leaks sessions or otherwise exceeds the limit, then as long as the client uses an appropriate originator argument, it will be easily identifiable from the XenServer logs and XenServer will destroy the longest-idle sessions of the rogue client only; this may cause problems for that client but not for other clients. If the misbehaving client did not specify an originator, it would be harder to identify and would cause the premature destruction of sessions of any clients that also did not specify an originator", 
            "title": "3.1.5. Logging out"
        }, 
        {
            "location": "/#316-install-and-start-example-summary", 
            "text": "We have seen how the API can be used to install a VM from a XenServer template and perform a number of lifecycle operations on it. You will note that the number of calls we had to make in order to affect these operations was small:    One call to acquire a session: Session.login_with_password()    One call to query the VM (and template) objects present on the XenServer installation: VM.get_all_records(). Recall that we used the information returned from this call to select a suitable template to install from.    Two calls to install a VM from our chosen template: VM.clone(), followed by VM.provision().    One call to start the resultant VM: VM.start() (and similarly other single calls to suspend, resume and shutdown accordingly)    And then one call to logout Session.logout()    The take-home message here is that, although the API as a whole is complex and fully featured, common tasks (such as creating and performing lifecycle operations on VMs) are very straightforward to perform, requiring only a small number of simple API calls. Keep this in mind while you study the next section which may, on first reading, appear a little daunting!", 
            "title": "3.1.6. Install and start example: summary"
        }, 
        {
            "location": "/#32-object-model-overview", 
            "text": "This section gives a high-level overview of the object model of the API. A more detailed description of the parameters and methods of each class outlined here can be found in the XenServer API Reference document.  We start by giving a brief outline of some of the core classes that make up the API. (Don't worry if these definitions seem somewhat abstract in their initial presentation; the textual description in subsequent sections, and the code-sample walk through in the next Chapter will help make these concepts concrete.)           VM  A VM object represents a particular virtual machine instance on a XenServer Host or Resource Pool. Example methods include start, suspend, pool_migrate; example parameters include power_state, memory_static_max, and name_label. (In the previous section we saw how the VM class is used to represent both templates and regular VMs)    Host  A host object represents a physical host in a XenServer pool. Example methods include reboot and shutdown. Example parameters include software_version, hostname, and [IP] address.    VDI  A VDI object represents a Virtual Disk Image. Virtual Disk Images can be attached to VMs, in which case a block device appears inside the VM through which the bits encapsulated by the Virtual Disk Image can be read and written. Example methods of the VDI class include \"resize\" and \"clone\". Example fields include \"virtual_size\" and \"sharable\". (When we called VM.provision on the VM template in our previous example, some VDI objects were automatically created to represent the newly created disks, and attached to the VM object.)    SR  An SR (Storage Repository) aggregates a collection of VDIs and encapsulates the properties of physical storage on which the VDIs' bits reside. Example parameters include type (which determines the storage-specific driver a XenServer installation uses to read/write the SR's VDIs) and physical_utilisation; example methods include scan(which invokes the storage-specific driver to acquire a list of the VDIs contained with the SR and the properties of these VDIs) and create (which initializes a block of physical storage so it is ready to store VDIs).    Network  A network object represents a layer-2 network that exists in the environment in which the XenServer Host instance lives. Since XenServer does not manage networks directly this is a lightweight class that serves merely to model physical and virtual network topology. VM and Host objects that are attached to a particular Network object (by virtue of VIF and PIF instances -- see below) can send network packets to each other.     At this point, readers who are finding this enumeration of classes rather terse may wish to skip to the code walk-throughs of the next chapter: there are plenty of useful applications that can be written using only a subset of the classes already described! For those who wish to continue this description of classes in the abstract, read on.  On top of the classes listed above, there are 4 more that act as connectors, specifying relationships between VMs and Hosts, and Storage and Networks. The first 2 of these classes that we will consider, VBD and VIF, determine how VMs are attached to virtual disks and network objects respectively:           VBD  A VBD (Virtual Block Device) object represents an attachment between a VM and a VDI. When a VM is booted its VBD objects are queried to determine which disk images (VDIs) should be attached. Example methods of the VBD class include \"plug\" (which hot plugs a disk device into a running VM, making the specified VDI accessible therein) and \"unplug\" (which hot unplugs a disk device from a running guest); example fields include \"device\" (which determines the device name inside the guest under which the specified VDI will be made accessible).    VIF  A VIF (Virtual network InterFace) object represents an attachment between a VM and a Network object. When a VM is booted its VIF objects are queried to determine which network devices should be created. Example methods of the VIF class include \"plug\" (which hot plugs a network device into a running VM) and \"unplug\" (which hot unplugs a network device from a running guest).     The second set of \"connector classes\" that we will consider determine how Hosts are attached to Networks and Storage.\n|||\n|-|-|\n| PIF  | A PIF (Physical InterFace) object represents an attachment between a Host and a Network object. If a host is connected to a Network (over a PIF) then packets from the specified host can be transmitted/received by the corresponding host. Example fields of the PIF class include \"device\" (which specifies the device name to which the PIF corresponds -- e.g. eth0) and \"MAC\" (which specifies the MAC address of the underlying NIC that a PIF represents). Note that PIFs abstract both physical interfaces and VLANs (the latter distinguished by the existence of a positive integer in the \"VLAN\" field).|\n| PBD   | A PBD (Physical Block Device) object represents an attachment between a Host and a SR (Storage Repository) object. Fields include \"currently-attached\" (which specifies whether the chunk of storage represented by the specified SR object) is currently available to the host; and \"device_config\" (which specifies storage-driver specific parameters that determines how the low-level storage devices are configured on the specified host -- e.g. in the case of an SR rendered on an NFS filer, device_config may specify the host-name of the filer and the path on the filer in which the SR files live.)|  Figure 3.1. Common API Classes   Graphical overview of API classes for managing VMs, Hosts, Storage and Networking  Figure 3.1, \u00a1\u00b0Common API Classes\" presents a graphical overview of the API classes involved in managing VMs, Hosts, Storage and Networking. From this diagram, the symmetry between storage and network configuration, and also the symmetry between virtual machine and host configuration is plain to see.", 
            "title": "3.2. Object Model Overview"
        }, 
        {
            "location": "/#33-working-with-vifs-and-vbds", 
            "text": "In this section we walk through a few more complex scenarios, describing informally how various tasks involving virtual storage and network devices can be accomplished using the API.", 
            "title": "3.3. Working with VIFs and VBDs"
        }, 
        {
            "location": "/#331-creating-disks-and-attaching-them-to-vms", 
            "text": "Let's start by considering how to make a new blank disk image and attach it to a running VM. We will assume that we already have ourselves a running VM, and we know its corresponding API object reference (e.g. we may have created this VM using the procedure described in the previous section, and had the server return its reference to us.) We will also assume that we have authenticated with the XenServer installation and have a corresponding session reference. Indeed in the rest of this chapter, for the sake of brevity, we will stop mentioning sessions altogether.", 
            "title": "3.3.1. Creating disks and attaching them to VMs"
        }, 
        {
            "location": "/#3311-creating-a-new-blank-disk-image", 
            "text": "The first step is to instantiate the disk image on physical storage. We do this by calling VDI.create(). The VDI.create call takes a number of parameters, including:    name_label and name_description: a human-readable name/description for the disk (e.g. for convenient display in the UI etc.). These fields can be left blank if desired.    SR: the object reference of the Storage Repository representing the physical storage in which the VDI's bits will be placed.    read_only: setting this field to true indicates that the VDI can only be attached to VMs in a read-only fashion. (Attempting to attach a VDI with its read_only field set to true in a read/write fashion results in error.)    Invoking the VDI.create call causes the XenServer installation to create a blank disk image on physical storage, create an associated VDI object (the datamodel instance that refers to the disk image on physical storage) and return a reference to this newly created VDI object.  The way in which the disk image is represented on physical storage depends on the type of the SR in which the created VDI resides. For example, if the SR is of type \"lvm\" then the new disk image will be rendered as an LVM volume; if the SR is of type \"nfs\" then the new disk image will be a sparse VHD file created on an NFS filer. (You can query the SR type through the API using the SR.get_type() call.)   Note \nSome SR types might round up the virtual-size value to make it divisible by a configured block size.", 
            "title": "3.3.1.1. Creating a new blank disk image"
        }, 
        {
            "location": "/#3312-attaching-the-disk-image-to-a-vm", 
            "text": "So far we have a running VM (that we assumed the existence of at the start of this example) and a fresh VDI that we just created. Right now, these are both independent objects that exist on the XenServer Host, but there is nothing linking them together. So our next step is to create such a link, associating the VDI with our VM.  The attachment is formed by creating a new \"connector\" object called a VBD (Virtual Block Device). To create our VBD we invoke the VBD.create() call. The VBD.create() call takes a number of parameters including:    VM - the object reference of the VM to which the VDI is to be attached    VDI - the object reference of the VDI that is to be attached    mode - specifies whether the VDI is to be attached in a read-only or a read-write fashion    userdevice - specifies the block device inside the guest through which applications running inside the VM will be able to read/write the VDI's bits.    type - specifies whether the VDI should be presented inside the VM as a regular disk or as a CD. (Note that this particular field has more meaning for Windows VMs than it does for Linux VMs, but we will not explore this level of detail in this chapter.)    Invoking VBD.create makes a VBD object on the XenServer installation and returns its object reference. However, this call in itself does not have any side-effects on the running VM (that is, if you go and look inside the running VM you will see that the block device has not been created). The fact that the VBD object exists but that the block device in the guest is not active, is reflected by the fact that the VBD object's currently_attached field is set to false.  Figure 3.2. A VM object with 2 associated VDIs   A VM object with 2 associated VDIs  For expository purposes, Figure 3.2, \u00a1\u00b0A VM object with 2 associated VDIs??? presents a graphical example that shows the relationship between VMs, VBDs, VDIs and SRs. In this instance a VM object has 2 attached VDIs: there are 2 VBD objects that form the connections between the VM object and its VDIs; and the VDIs reside within the same SR.", 
            "title": "3.3.1.2. Attaching the disk image to a VM"
        }, 
        {
            "location": "/#3313-hotplugging-the-vbd", 
            "text": "If we rebooted the VM at this stage then, after rebooting, the block device corresponding to the VBD would appear: on boot, XenServer queries all VBDs of a VM and actively attaches each of the corresponding VDIs.  Rebooting the VM is all very well, but recall that we wanted to attach a newly created blank disk to a running VM. This can be achieved by invoking the plug method on the newly created VBD object. When the plug call returns successfully, the block device to which the VBD relates will have appeared inside the running VM -- i.e. from the perspective of the running VM, the guest operating system is led to believe that a new disk device has just been hot plugged. Mirroring this fact in the managed world of the API, the currently_attached field of the VBD is set to true.  Unsurprisingly, the VBD plug method has a dual called \"unplug\". Invoking the unplug method on a VBD object causes the associated block device to be hot unplugged from a running VM, setting the currently_attached field of the VBD object to false accordingly.", 
            "title": "3.3.1.3. Hotplugging the VBD"
        }, 
        {
            "location": "/#332-creating-and-attaching-network-devices-to-vms", 
            "text": "The API calls involved in configuring virtual network interfaces in VMs are similar in many respects to the calls involved in configuring virtual disk devices. For this reason we will not run through a full example of how one can create network interfaces using the API object-model; instead we will use this section just to outline briefly the symmetry between virtual networking device and virtual storage device configuration.  The networking analogue of the VBD class is the VIF class. Just as a VBD is the API representation of a block device inside a VM, a VIF (Virtual network InterFace) is the API representation of a network device inside a VM. Whereas VBDs associate VM objects with VDI objects, VIFs associate VM objects with Network objects. Just like VBDs, VIFs have a currently_attached field that determines whether or not the network device (inside the guest) associated with the VIF is currently active or not. And as we saw with VBDs, at VM boot-time the VIFs of the VM are queried and a corresponding network device for each created inside the booting VM. Similarly, VIFs also have plug and unplug methods for hot plugging/unplugging network devices in/out of running VMs.", 
            "title": "3.3.2. Creating and attaching Network Devices to VMs"
        }, 
        {
            "location": "/#333-host-configuration-for-networking-and-storage", 
            "text": "We have seen that the VBD and VIF classes are used to manage configuration of block devices and network devices (respectively) inside VMs. To manage host configuration of storage and networking there are two analogous classes: PBD (Physical Block Device) and PIF (Physical [network] InterFace).", 
            "title": "3.3.3. Host configuration for networking and storage"
        }, 
        {
            "location": "/#3331-host-storage-configuration-pbds", 
            "text": "Let us start by considering the PBD class. A PBD_create() call takes a number of parameters including:     Parameter  Description      host  physical machine on which the PBD is available    SR  the Storage Repository that the PBD connects to    device_config  a string-to-string map that is provided to the host's SR-backend-driver, containing the low-level parameters required to configure the physical storage device(s) on which the SR is to be realized. The specific contents of the device_config field depend on the type of the SR to which the PBD is connected. (Executing xe sm-listwill show a list of possible SR types; the configuration field in this enumeration specifies the device_config parameters that each SR type expects.)     For example, imagine we have an SR object s of type \"nfs\" (representing a directory on an NFS filer within which VDIs are stored as VHD files); and let's say that we want a host, h, to be able to access s. In this case we invoke PBD.create() specifying host h, SR s, and a value for the device_config parameter that is the following map:  (\"server\", \"my_nfs_server.example.com\"), (\"serverpath\", \"/scratch/mysrs/sr1\")  This tells the XenServer Host that SR s is accessible on host h, and further that to access SR s, the host needs to mount the directory /scratch/mysrs/sr1 on the NFS server named my_nfs_server.example.com.  Like VBD objects, PBD objects also have a field called currently_attached. Storage repositories can be attached and detached from a given host by invoking PBD.plug and PBD.unplug methods respectively.", 
            "title": "3.3.3.1. Host storage configuration: PBDs"
        }, 
        {
            "location": "/#3332-host-networking-configuration-pifs", 
            "text": "Host network configuration is specified by virtue of PIF objects. If a PIF object connects a network object, n, to a host object h, then the network corresponding to n is bridged onto a physical interface (or a physical interface plus a VLAN tag) specified by the fields of the PIF object.  For example, imagine a PIF object exists connecting host h to a network n, and that device field of the PIF object is set to eth0. This means that all packets on network n are bridged to the NIC in the host corresponding to host network device eth0.", 
            "title": "3.3.3.2. Host networking configuration: PIFs"
        }, 
        {
            "location": "/#34-exporting-and-importing-vms", 
            "text": "VMs can be exported to a file and later imported to any XenServer host. The export protocol is a simple HTTP(S) GET, which should be performed on the master if the VM is on a pool member. Authorization is either standard HTTP basic authentication, or if a session has already been obtained, this can be used. The VM to export is specified either by UUID or by reference. To keep track of the export, a task can be created and passed in using its reference. The request might result in a redirect if the VM's disks are only accessible on a pool member.  The following arguments are passed on the command line:     Argument  Description      session_id  the reference of the session being used to authenticate; required only when not using HTTP basic authentication    task_id  the reference of the task object with which to keep track of the operation; optional, required only if you have created a task object to keep track of the export    ref  the reference of the VM; required only if not using the UUID    uuid  the UUID of the VM; required only if not using the reference     For example, using the Linux command line tool cURL:  curl http://root:foo@myxenserver1/export?uuid= vm_uuid  -o  exportfile  will export the specified VM to the file exportfile.  To export just the metadata, use the URI http://server/export_metadata.  The import protocol is similar, using HTTP(S) PUT. The session_id and task_id arguments are as for the export. The ref and uuid are not used; a new reference and uuid will be generated for the VM. There are some additional parameters:     Argument  Description      restore  if true, the import is treated as replacing the original VM - the implication of this currently is that the MAC addresses on the VIFs are exactly as the export was, which will lead to conflicts if the original VM is still being run.    force  if true, any checksum failures will be ignored (the default is to destroy the VM if a checksum error is detected)    sr_id  the reference of an SR into which the VM should be imported. The default behavior is to import into the Pool.default_SR.     For example, again using cURL:  curl -T  exportfile  http://root:foo@myxenserver2/import \nwill import the VM to the default SR on the server.   Note \nNote that if no default SR has been set, and no sr_uuid is specified, the error message \"DEFAULT_SR_NOT_FOUND\" is returned   Another example:  curl -T  exportfile  http://root:foo@myxenserver2/import?sr_id= opaque_ref_of_sr \nwill import the VM to the specified SR on the server.  To import just the metadata, use the URI http://server/import_metadata", 
            "title": "3.4. Exporting and Importing VMs"
        }, 
        {
            "location": "/#341-xen-virtual-appliance-xva-vm-import-format", 
            "text": "XenServer supports a human-readable legacy VM input format called XVA. This section describes the syntax and structure of XVA.  An XVA consists of a directory containing XML metadata and a set of disk images. A VM represented by an XVA is not intended to be directly executable. Data within an XVA package is compressed and intended for either archiving on permanent storage or for being transmitted to a VM server - such as a XenServer host - where it can be decompressed and executed.  XVA is a hypervisor-neutral packaging format; it should be possible to create simple tools to instantiate an XVA VM on any other platform. XVA does not specify any particular runtime format; for example disks may be instantiated as file images, LVM volumes, QCoW images, VMDK or VHD images. An XVA VM may be instantiated any number of times, each instantiation may have a different runtime format.  XVA does not:    specify any particular serialization or transport format    provide any mechanism for customizing VMs (or templates) on install    address how a VM may be upgraded post-install    define how multiple VMs, acting as an appliance, may communicate    These issues are all addressed by the related Open Virtual Appliance specification.  An XVA is a directory containing, at a minimum, a file called ova.xml. This file describes the VM contained within the XVA and is described in Section 3.2. Disks are stored within sub-directories and are referenced from the ova.xml. The format of disk data is described later in Section 3.3.  The following terms will be used in the rest of the chapter:    HVM: a mode in which unmodified OS kernels run with the help of virtualization support in the hardware.    PV: a mode in which specially modified \"paravirtualized\" kernels run explicitly on top of a hypervisor without requiring hardware support for virtualization.    The  \"ova.xml\"  file contains the following elements:  appliance version=\"0.1\" \nThe number in the attribute \"version\" indicates the version of this specification to which the XVA is constructed; in this case version 0.1. Inside the   there is exactly one  : (in the OVA specification, multiple  s are permitted)  vm name=\"name\" \nEach   element describes one VM. The \"name\" attribute is for future internal use only and must be unique within the ova.xml file. The \"name\" attribute is permitted to be any valid UTF-8 string. Inside each   tag are the following compulsory elements:  label ... text ...  /label \nA short name for the VM to be displayed in a UI.  shortdesc  ... description ...  /shortdesc \nA description for the VM to be displayed in the UI. Note that for both  label  and  shortdesc  contents, leading and trailing whitespace will be ignored.  config mem_set=\"268435456\" vcpus=\"1\"/ \nThe   element has attributes which describe the amount of memory in bytes (mem_set) and number of CPUs (VCPUs) the VM should have.  Each   has zero or more  vbd  elements representing block devices which look like the following:  vbd device=\"sda\" function=\"root\" mode=\"w\" vdi=\"vdi_sda\"/ \nThe attributes have the following meanings:    device\n    name of the physical device to expose to the VM. For linux guests we use \"sd[a-z]\" and for windows guests we use \"hd[a-d]\".    function\nif marked as \"root\", this disk will be used to boot the guest. (NB this does not imply the existence of the Linux root i.e. / filesystem) Only one device should be marked as \"root\". See Section 3.4 describing VM booting. Any other string is ignored.    mode\neither \"w\" or \"ro\" if the device is to be read/write or read-only    vdi\nthe name of the disk image (represented by a   element) to which this block device is connected    Each  vm  may have an optional  hacks  section like the following:  hacks is_hvm=\"false\" kernel_boot_cmdline=\"root=/dev/sda1 ro\"/  The  hacks  element is present in the XVA files generated by XenServer but will be removed in future. The attribute \"is_hvm\" is either \"true\" or \"false\", depending on whether the VM should be booted in HVM or not. The \"kernel_boot_cmdline\" contains additional kernel commandline arguments when booting a guest using pygrub.  In addition to a  vm  element, the  appliance  will contain zero or more  vdi  elements like the following:  vdi name=\"vdi_sda\" size=\"5368709120\" source=\"file://sda\"\ntype=\"dir-gzipped-chunks\" \nEach  vdi  corresponds to a disk image. The attributes have the following meanings:  name: name of the VDI, referenced by the vdi attribute of  vbd  elements. Any valid UTF-8 string is permitted.  size: size of the required image in bytes  source: a URI describing where to find the data for the image, only file:// URIs are currently permitted and must describe paths relative to the directory containing the ova.xml  type: describes the format of the disk data (see Section 3.3)  A single disk image encoding is specified in which has type \"dir-gzipped-chunks\": Each image is represented by a directory containing a sequence of files as follows:  -rw-r--r-- 1 dscott xendev 458286013    Sep 18 09:51 chunk000000000.gz\n-rw-r--r-- 1 dscott xendev 422271283    Sep 18 09:52 chunk000000001.gz\n-rw-r--r-- 1 dscott xendev 395914244    Sep 18 09:53 chunk000000002.gz\n-rw-r--r-- 1 dscott xendev 9452401      Sep 18 09:53 chunk000000003.gz\n-rw-r--r-- 1 dscott xendev 1096066      Sep 18 09:53 chunk000000004.gz\n-rw-r--r-- 1 dscott xendev 971976       Sep 18 09:53 chunk000000005.gz\n-rw-r--r-- 1 dscott xendev 971976       Sep 18 09:53 chunk000000006.gz\n-rw-r--r-- 1 dscott xendev 971976       Sep 18 09:53 chunk000000007.gz\n-rw-r--r-- 1 dscott xendev 573930       Sep 18 09:53 chunk000000008.gz  Each file (named  \"chunk-XXXXXXXXX.gz\" ) is a gzipped file containing exactly 1e9 bytes (1GB, not 1GiB) of raw block data. The small size was chosen to be safely under the maximum file size limits of several filesystems. If the files are gunzipped and then concatenated together, the original image is recovered.  XenServer provides two mechanisms for booting a VM: (i) using a paravirtualized kernel extracted through pygrub; and (ii) using HVM. The current implementation uses the \"is_hvm\" flag within the   section to decide which mechanism to use.  This rest of this section describes a very simple Debian VM packaged as an XVA. The VM has two disks, one with size 5120MiB and used for the root filesystem and used to boot the guest using pygrub and the other of size 512MiB which is used for swap. The VM has 512MiB of memory and uses one virtual CPU.  At the topmost level the simple Debian VM is represented by a single directory:  $ ls -l\ntotal 4\ndrwxr-xr-x 3 dscott xendev 4096 Oct 24 09:42 very simple Debian VM  Inside the main XVA directory are two sub-directories - one per disk - and the single file: ova.xml:  $ ls -l very\\ simple\\ Debian\\ VM/\ntotal 8\n-rw-r--r-- 1 dscott xendev 1016 Oct 24 09:42 ova.xml\ndrwxr-xr-x 2 dscott xendev 4096 Oct 24 09:42 sda\ndrwxr-xr-x 2 dscott xendev 4096 Oct 24 09:53 sdb  Inside each disk sub-directory are a set of files, each file contains 1GB of raw disk block data compressed using gzip:  $ ls -l very\\ simple\\ Debian\\ VM/sda/\ntotal 2053480\n-rw-r--r-- 1 dscott xendev 202121645 Oct 24 09:43 chunk-000000000.gz\n-rw-r--r-- 1 dscott xendev 332739042 Oct 24 09:45 chunk-000000001.gz\n-rw-r--r-- 1 dscott xendev 401299288 Oct 24 09:48 chunk-000000002.gz\n-rw-r--r-- 1 dscott xendev 389585534 Oct 24 09:50 chunk-000000003.gz\n-rw-r--r-- 1 dscott xendev 624567877 Oct 24 09:53 chunk-000000004.gz\n-rw-r--r-- 1 dscott xendev 150351797 Oct 24 09:54 chunk-000000005.gz\n$ ls -l very\\ simple\\ Debian\\ VM/sdb\ntotal 516\n-rw-r--r-- 1 dscott xendev 521937 Oct 24 09:54 chunk-000000000.gz  The example simple Debian VM would have an XVA file like the following:  ?xml version=\"1.0\" ?  appliance version=\"0.1\" \n   vm name=\"vm\" \n   label \n  very simple Debian VM\n   /label \n   shortdesc \n  the description field can contain any valid UTF-8\n   /shortdesc \n   config mem_set=\"536870912\" vcpus=\"1\"/ \n   hacks is_hvm=\"false\" kernel_boot_cmdline=\"root=/dev/sda1 ro \" \n   !--This section is temporary and will be ignored in future. Attribute\nis_hvm (\"true\" or \"false\") indicates whether the VM will be booted in HVM mode. In\nfuture this will be autodetected. Attribute kernel_boot_cmdline contains the kernel\ncommandline for the case where a proper grub menu.lst is not present. In future\nbooting shall only use pygrub.-- \n   /hacks \n   vbd device=\"sda\" function=\"root\" mode=\"w\" vdi=\"vdi_sda\"/ \n   vbd device=\"sdb\" function=\"swap\" mode=\"w\" vdi=\"vdi_sdb\"/ \n   /vm \n   vdi name=\"vdi_sda\" size=\"5368709120\" source=\"file://sda\" type=\"dir-gzippedchunks\"/ \n   vdi name=\"vdi_sdb\" size=\"536870912\" source=\"file://sdb\" type=\"dir-gzippedchunks\"/  /appliance", 
            "title": "3.4.1. Xen Virtual Appliance (XVA) VM Import Format"
        }, 
        {
            "location": "/#35-xml-rpc-notes", 
            "text": "", 
            "title": "3.5. XML-RPC notes"
        }, 
        {
            "location": "/#351-datetimes", 
            "text": "The API deviates from the XML-RPC specification in handling of datetimes. The API appends a \"Z\" to the end of datetime strings, which is meant to indicate that the time is expressed in UTC.", 
            "title": "3.5.1. Datetimes"
        }, 
        {
            "location": "/#36-where-to-look-next", 
            "text": "In this chapter we have presented a brief high-level overview of the API and its object-model. The aim here is not to present the detailed semantics of the API, but just to provide enough background for you to start reading the code samples of the next chapter and to find your way around the more detailed XenServer API Reference reference document.  There are a number of places you can find more information:  The XenServer Administrators Guide contains an overview of the xe CLI. Since a good deal of xe commands are a thin veneer over the API, playing with xe is a good way to start finding your way around the API object model described in this chapter.  The code samples in the next chapter provide some concrete instances of API coding in a variety of client languages.  The  XenServer API Reference reference  document provides a more detailed description of the API semantics as well as describing the format of XML/RPC messages on the wire.  There are a few scripts that use the API in the XenServer Host dom0 itself. For example, \"/opt/xensource/libexec/shutdown\" is a python program that cleanly shuts VMs down. This script is invoked when the host itself is shut down.", 
            "title": "3.6. Where to look next"
        }, 
        {
            "location": "/#chapter-4-using-the-api", 
            "text": "This chapter describes how to use the XenServer Management API from real programs to manage XenServer Hosts and VMs. The chapter begins with a walk-through of a typical client application and demonstrates how the API can be used to perform common tasks. Example code fragments are given in python syntax but equivalent code in the other programming languages would look very similar. The language bindings themselves are discussed afterwards and the chapter finishes with walk-throughs of two complete examples.", 
            "title": "Chapter 4. Using the API"
        }, 
        {
            "location": "/#41-anatomy-of-a-typical-application", 
            "text": "This section describes the structure of a typical application using the XenServer Management API. Most client applications begin by connecting to a XenServer Host and authenticating (e.g. with a username and password). Assuming the authentication succeeds, the server will create a \"session\" object and return a reference to the client. This reference will be passed as an argument to all future API calls. Once authenticated, the client may search for references to other useful objects (e.g. XenServer Hosts, VMs, etc.) and invoke operations on them. Operations may be invoked either synchronously or asynchronously; special task objects represent the state and progress of asynchronous operations. These application elements are all described in detail in the following sections.", 
            "title": "4.1. Anatomy of a typical application"
        }, 
        {
            "location": "/#411-choosing-a-low-level-transport", 
            "text": "API calls can be issued over two transports:    SSL-encrypted TCP on port 443 (https) over an IP network    plaintext over a local Unix domain socket: /var/xapi/xapi    The SSL-encrypted TCP transport is used for all off-host traffic while the Unix domain socket can be used from services running directly on the XenServer Host itself. In the SSL-encrypted TCP transport, all API calls should be directed at the Resource Pool master; failure to do so will result in the error HOST_IS_SLAVE, which includes the IP address of the master as an error parameter.  Because the master host of a pool can change, especially if HA is enabled on a pool, clients must implement the following steps to detect a master host change and connect to the new master as required:  Handling pool master changes    Subscribe to updates in the list of hosts servers, and maintain a current list of hosts in the pool    If the connection to the pool master fails to respond, attempt to connect to all hosts in the list until one responds    The first host to respond will return the HOST_IS_SLAVE error message, which contains the identity of the new pool master (unless of course the host is the new master)    Connect to the new master     Note \nAs a special-case, all messages sent through the Unix domain socket are transparently forwarded to the correct node.", 
            "title": "4.1.1. Choosing a low-level transport"
        }, 
        {
            "location": "/#412-authentication-and-session-handling", 
            "text": "The vast majority of API calls take a session reference as their first parameter; failure to supply a valid reference will result in a SESSION_INVALID error being returned. Acquire a session reference by supplying a username and password to the login_with_password function.   Note \nAs a special-case, if this call is executed over the local Unix domain socket then the username and password are ignored and the call always succeeds   Every session has an associated \"last active\" timestamp which is updated on every API call. The server software currently has a built-in limit of 500 active sessions and will remove those with the oldest \"last active\" field if this limit is exceeded for a given username or originator. In addition all sessions whose \"last active\" field is older than 24 hours are also removed. Therefore it is important to:    Specify an appropriate originator when logging in; and    Remember to log out of active sessions to avoid leaking them; and    Be prepared to log in again to the server if a SESSION_INVALID error is caught.    In the following Python fragment a connection is established over the Unix domain socket and a session is created:  import   XenAPI \n\n     session   =   XenAPI . xapi_local () \n     try : \n         session . xenapi . login_with_password ( root ,   ,   2.3 ,   My Widget v0.1 ) \n         ... \n     finally : \n         session . xenapi . session . logout ()", 
            "title": "4.1.2. Authentication and session handling"
        }, 
        {
            "location": "/#413-finding-references-to-useful-objects", 
            "text": "Once an application has authenticated the next step is to acquire references to objects in order to query their state or invoke operations on them. All objects have a set of \"implicit\" messages which include the following:    get_by_name_label : return a list of all objects of a particular class with a particular label;    get_by_uuid : return a single object named by its UUID;    get_all : return a set of references to all objects of a particular class; and    get_all_records : return a map of reference to records for each object of a particular class.    For example, to list all hosts:  hosts = session.xenapi.host.get_all() \nTo find all VMs with the name \"my first VM\":  vms = session.xenapi.VM.get_by_name_label('my first VM')   Note \nObject name_label fields are not guaranteed to be unique and so the get_by_name_label API call returns a set of references rather than a single reference.   In addition to the methods of finding objects described above, most objects also contain references to other objects within fields. For example it is possible to find the set of VMs running on a particular host by calling:  vms = session.xenapi.host.get_resident_VMs(host)", 
            "title": "4.1.3. Finding references to useful objects"
        }, 
        {
            "location": "/#414-invoking-synchronous-operations-on-objects", 
            "text": "Once object references have been acquired, operations may be invoked on them. For example to start a VM:  session.xenapi.VM.start(vm, False, False) \nAll API calls are by default synchronous and will not return until the operation has completed or failed. For example in the case of VM.start the call does not return until the VM has started booting.   Note \nWhen the VM.start call returns the VM will be booting. To determine when the booting has finished, wait for the in-guest agent to report internal statistics through the VM_guest_metrics object.", 
            "title": "4.1.4. Invoking synchronous operations on objects"
        }, 
        {
            "location": "/#415-using-tasks-to-manage-asynchronous-operations", 
            "text": "To simplify managing operations which take quite a long time (e.g. VM.clone and VM.copy) functions are available in two forms: synchronous (the default) and asynchronous. Each asynchronous function returns a reference to a task object which contains information   about the in-progress operation including:    whether it is pending    whether it is has succeeded or failed    progress (in the range 0-1)    the result or error code returned by the operation    An application which wanted to track the progress of a VM.clone operation and display a progress bar would have code like the following:  vm = session.xenapi.VM.get_by_name_label( my vm )\ntask = session.xenapi.Async.VM.clone(vm)\nwhile session.xenapi.task.get_status(task) ==  pending :\n        progress = session.xenapi.task.get_progress(task)\n        update_progress_bar(progress)\n        time.sleep(1)\nsession.xenapi.task.destroy(task)   Note \nNote that a well-behaved client should remember to delete tasks created by asynchronous operations when it has finished reading the result or error. If the number of tasks exceeds a built-in threshold then the server will delete the oldest of the completed tasks.", 
            "title": "4.1.5. Using Tasks to manage asynchronous operations"
        }, 
        {
            "location": "/#416-subscribing-to-and-listening-for-events", 
            "text": "With the exception of the task and metrics classes, whenever an object is modified the server generates an event. Clients can subscribe to this event stream on a per-class basis and receive updates rather than resorting to frequent polling. Events come in three types:    add - generated when an object has been created;    del - generated immediately before an object is destroyed; and    mod - generated when an object's field has changed.    Events also contain a monotonically increasing ID, the name of the class of object and a snapshot of the object state equivalent to the result of a get_record().  Clients register for events by calling event.register() with a list of class names or the special string \"*\". Clients receive events by executing event.next() which blocks until events are available and returns the new events.   Note \nSince the queue of generated events on the server is of finite length a very slow client might fail to read the events fast enough; if this happens an EVENTS_LOST error is returned. Clients should be prepared to handle this by re-registering for events and checking that the condition they are waiting for hasn't become true while they were unregistered.   The following python code fragment demonstrates how to print a summary of every event generated by a system: (similar code exists in  Xenserver-SDK/XenServerPython/samples/watch-all-events.py )  fmt =  %8s  %20s  %5s  %s \nsession.xenapi.event.register([ * ])\nwhile True:\n    try:\n        for event in session.xenapi.event.next():\n            name =  (unknown) \n            if  snapshot  in event.keys():\n                snapshot = event[ snapshot ]\n                if  name_label  in snapshot.keys():\n                    name = snapshot[ name_label ]\n            print fmt % (event[ id ], event[ class ], event[ operation ], name)           \n    except XenAPI.Failure, e:\n        if e.details == [  EVENTS_LOST  ]:\n            print  Caught EVENTS_LOST; should reregister", 
            "title": "4.1.6. Subscribing to and listening for events"
        }, 
        {
            "location": "/#42-language-bindings", 
            "text": "", 
            "title": "4.2. Language bindings"
        }, 
        {
            "location": "/#421-c", 
            "text": "The SDK includes the source to the C language binding in the directory XenServer-SDK/libxenserver/src together with a Makefile which compiles the binding into a library. Every API object is associated with a header file which contains declarations for all that object's API functions; for example the type definitions and functions required to invoke VM operations are all contained in xen_vm.h.  C binding dependencies \n|||\n-------------|---------------|\n|Platform|supported:    Linux|\n|Library:|The language binding is generated as a libxenserver.so that is linked by C programs.|\n|Dependencies:| XML library (libxml2.so on GNU Linux) Curl library (libcurl2.so) |  The following simple examples are included with the C bindings:    test_vm_async_migrate: demonstrates how to use asynchronous API calls to migrate running VMs from a slave host to the pool master.    test_vm_ops: demonstrates how to query the capabilities of a host, create a VM, attach a fresh blank disk image to the VM and then perform various powercycle operations;    test_failures: demonstrates how to translate error strings into enum_xen_api_failure, and vice versa;    test_event_handling: demonstrates how to listen for events on a connection.    test_enumerate: demonstrates how to enumerate the various API objects.", 
            "title": "4.2.1. C"
        }, 
        {
            "location": "/#422-c", 
            "text": "The C# bindings are contained within the directory XenServer-SDK/XenServer.NET and include project files suitable for building under Microsoft Visual Studio. Every API object is associated with one C# file; for example the functions implementing the VM operations are contained within the file VM.cs.  C# binding dependencies           Platform supported:  Windows with .NET version 4.5    Library:  The language binding is generated as a Dynamic Link Library XenServer.dll that is linked by C# programs.    Dependencies:  CookComputing.XMLRpcV2.dll is needed for the XenServer.dll to be able to communicate with the xml-rpc server. We ship a patched 2.1.0.6 version and recommend that you use this one, though others may work.     Three examples are included with the C# bindings in the directory XenServer-SDK/XneServer.NET/samples as separate projects of the XenSdkSample.sln solution:    GetVariousRecords: logs into a XenServer Host and displays information about hosts, storage and virtual machines;    GetVmRecords: logs into a XenServer Host and lists all the VM records;    VmPowerStates: logs into a XenServer Host, finds a VM and takes it through the various power states. Requires a shut-down VM to be already installed.", 
            "title": "4.2.2. C"
        }, 
        {
            "location": "/#423-java", 
            "text": "The Java bindings are contained within the directory XenServer-SDK/XenServerJava and include project files suitable for building under Microsoft Visual Studio. Every API object is associated with one Java file; for example the functions implementing the VM operations are contained within the file VM.java.  Java binding dependencies \n|||\n-------------|---------------|\n|Platform supported:|Linux and Windows|\n|Library:|  The language binding is generated as a Java Archive file xenserver-7.0.jar that is linked by Java programs.|\n|Dependencies:| xmlrpc-client-3.1.jar is needed for the xenserver.jar to be able to communicate with the xml-rpc server. ws-commons-util-1.0.2.jar is needed to run the examples. |  Running the main file XenServer-SDK/XenServerJava/samples/RunTests.java will run a series of examples included in the same directory:    AddNetwork: Adds a new internal network not attached to any NICs;    SessionReuse: Demonstrates how a Session object can be shared between multiple Connections;    AsyncVMCreate: Makes asynchronously a new VM from a built-in template, starts and stops it;    VdiAndSrOps: Performs various SR and VDI tests, including creating a dummy SR;    CreateVM: Creates a VM on the default SR with a network and DVD drive;    DeprecatedMethod: Tests a warning is displayed when a deprecated API method is called;    GetAllRecordsOfAllTypes: Retrieves all the records for all types of objects;    SharedStorage: Creates a shared NFS SR;    StartAllVMs: Connects to a host and tries to start each VM on it.", 
            "title": "4.2.3. Java"
        }, 
        {
            "location": "/#424-powershell", 
            "text": "The PowerShell bindings are contained within the directory XenServer-SDK/XenServerPowerShell. We provide the PowerShell module XenServerPSModule and source code exposing the XenServer API as Windows PowerShell cmdlets.  PowerShell binding dependencies \n|||\n-------------|---------------|\n|Platform supported:|Windows with .NET Framework 4.5 and PowerShell v4.0|\n|Library:|XenServerPSModule|\n|Dependencies:|CookComputing.XMLRpcV2.dll is needed to be able to communicate with the xml-rpc server. We ship a patched 2.1.0.6 version and recommend that you use this one, though others may work.|  The following example scripts are included with the PowerShell bindings in the directory XenServer-SDK/XenServerPowerShell/samples:    AutomatedTestCore.ps1: demonstrates how to log into a XenServer host, create a storage repository and a VM, and then perform various powercycle operations.    HttpTest.ps1: demonstrates how to log into a XenServer host, create a VM, and then perform operations such as VM importing and exporting, patch upload, and retrieval of performance statistics.", 
            "title": "4.2.4. PowerShell"
        }, 
        {
            "location": "/#425-python", 
            "text": "The python bindings are contained within a single file: XenServer-SDK/XenServerPython/XenAPI.py.  Python binding dependencies           Platform supported:  Linux    Library:  XenAPI.py    Dependencies:  None     The SDK includes 7 python examples:    fixpbds.py - reconfigures the settings used to access shared storage;    install.py - installs a Debian VM, connects it to a network, starts it up and waits for it to report its IP address;    license.py - uploads a fresh license to a XenServer Host;    permute.py - selects a set of VMs and uses XenMotion to move them simultaneously between hosts;    powercycle.py - selects a set of VMs and powercycles them;    shell.py - a simple interactive shell for testing;    vm_start_async.py - demonstrates how to invoke operations asynchronously;    watch-all-events.py - registers for all events and prints details when they occur.", 
            "title": "4.2.5. Python"
        }, 
        {
            "location": "/#426-command-line-interface-cli", 
            "text": "Besides using raw XML-RPC or one of the supplied language bindings, third-party software developers may integrate with XenServer Hosts by using the XE command line interface xe. The xe CLI is installed by default on XenServer hosts; a stand-alone remote CLI is also available for Linux. On Windows, the xe.exe CLI executable is installed along with XenCenter.  CLI dependencies           Platform supported:  Linux and Windows    Library:  None    Binary:  xe (xe.exe on Windows)    Dependencies:  None     The CLI allows almost every API call to be directly invoked from a script or other program, silently taking care of the required session management. The XE CLI syntax and capabilities are described in detail in the XenServer Administrator's Guide. For additional resources and examples, visit the  Citrix Knowledge Center .   Note \nWhen running the CLI from a XenServer Host console, tab-completion of both command names and arguments is available.", 
            "title": "4.2.6. Command Line Interface (CLI)"
        }, 
        {
            "location": "/#43-complete-application-examples", 
            "text": "This section describes two complete examples of real programs using the API.", 
            "title": "4.3. Complete application examples"
        }, 
        {
            "location": "/#431-simultaneously-migrating-vms-using-xenmotion", 
            "text": "This python example (contained in XenServer-SDK/XenServerPython/samples/permute.py) demonstrates how to use XenMotion to move VMs simultaneously between hosts in a Resource Pool. The example makes use of asynchronous API calls and shows how to wait for a set of tasks to complete.  The program begins with some standard boilerplate and imports the API bindings module  import   sys ,   time  import   XenAPI   Next the commandline arguments containing a server URL, username, password and a number of iterations are parsed. The username and password are used to establish a session which is passed to the function main, which is called multiple times in a loop. Note the use of try: finally: to make sure the program logs out of its session at the end.  if __name__ ==  __main__ :\n    if len(sys.argv)   5:\n        print  Usage: \n        print sys.argv[0],    url   username   password   iterations \n        sys.exit(1)\n    url = sys.argv[1]\n    username = sys.argv[2]\n    password = sys.argv[3]\n    iterations = int(sys.argv[4])\n    # First acquire a valid session by logging in:\n    session = XenAPI.Session(url)\n    session.xenapi.login_with_password(username, password,  2.3 ,\n                                        Example migration-demo v0.1 )\n    try:\n        for i in range(iterations):\n            main(session, i)\n    finally:\n        session.xenapi.session.logout()  The main function examines each running VM in the system, taking care to filter out control domains (which are part of the system and not controllable by the user). A list of running VMs and their current hosts is constructed.  def main(session, iteration):\n    # Find a non-template VM object\n    all = session.xenapi.VM.get_all()\n    vms = []\n    hosts = []\n    for vm in all:\n        record = session.xenapi.VM.get_record(vm)\n        if not(record[ is_a_template ]) and \\\n           not(record[ is_control_domain ]) and \\\n           record[ power_state ] ==  Running :\n            vms.append(vm)\n            hosts.append(record[ resident_on ])\n    print  %d: Found %d suitable running VMs  % (iteration, len(vms))  Next the list of hosts is rotated:  # use a rotation as a permutation\n    hosts = [hosts[-1]] + hosts[:(len(hosts)-1)]  Each VM is then moved using XenMotion to the new host under this rotation (i.e. a VM running on host at position 2 in the list will be moved to the host at position 1 in the list etc.) In order to execute each of the movements in parallel, the asynchronous version of the VM.pool_migrate is used and a list of task references constructed. Note the live flag passed to the VM.pool_migrate; this causes the VMs to be moved while they are still running.  tasks = []\n    for i in range(0, len(vms)):\n        vm = vms[i]\n        host = hosts[i]\n        task = session.xenapi.Async.VM.pool_migrate(vm, host, {  live :  true  })\n        tasks.append(task)  The list of tasks is then polled for completion:  finished = False\n    records = {}\n    while not(finished):\n        finished = True\n        for task in tasks:\n            record = session.xenapi.task.get_record(task)\n            records[task] = record\n            if record[ status ] ==  pending :\n                finished = False\n        time.sleep(1)  Once all tasks have left the pending state (i.e. they have successfully completed, failed or been cancelled) the tasks are polled once more to see if they all succeeded:  allok = True\n    for task in tasks:\n        record = records[task]\n        if record[ status ]    success :\n            allok = False  If any one of the tasks failed then details are printed, an exception is raised and the task objects left around for further inspection. If all tasks succeeded then the task objects are destroyed and the function returns.  if   not ( allok ): \n         print   One of the tasks didn t succeed at ,   \\ \n             time .strftime ( %F:%HT%M:%SZ ,   time .gmtime ()) \n         idx   =   0 \n         for   task   in   tasks : \n             record   =   records [ task ] \n             vm_name   =   session .xenapi.VM.get_name_label ( vms [ idx ] ) \n             host_name   =   session .xenapi.host.get_name_label ( hosts [ idx ] ) \n             print   %s : %12s %s -  %s  [   status :   % s ;   result  =   % s ;   error  =   % s   ]   %   \\ \n                   ( record [ uuid ] ,   record [ name_label ] ,   vm_name ,   host_name ,        \\ \n                    record [ status ] ,   record [ result ] ,   repr ( record [ error_info ] )) \n             idx   =   idx   +   1 \n         raise   Task failed \n     else : \n         for   task   in   tasks : \n             session .xenapi.task.destroy ( task )", 
            "title": "4.3.1. Simultaneously migrating VMs using XenMotion"
        }, 
        {
            "location": "/#432-cloning-a-vm-using-the-xe-cli", 
            "text": "This example is a bash script which uses the XE CLI to clone a VM   taking care to shut it down first if it is powered on.  The example begins with some boilerplate which first checks if the environment variable XE has been set: if it has it assumes that it points to the full path of the CLI, else it is assumed that the XE CLI is on the current path. Next the script prompts the user for a server name, username and password:  # Allow the path to the  xe  binary to be overridden by the XE environment variable\nif [ -z  ${ XE }  ]; then\n  XE=xe\nfi\n\nif [ ! -e  ${ HOME } /.xe  ]; then\n  read -p  Server name:   SERVER\n  read -p  Username:   USERNAME\n  read -p  Password:   PASSWORD\n  XE= ${ XE }  -s  ${ SERVER }  -u  ${ USERNAME }  -pw  ${ PASSWORD } \nfi  Next the script checks its commandline arguments. It requires exactly one: the UUID of the VM which is to be cloned:  # Check if there s a VM by the uuid specified ${ XE }  vm-list params=uuid | grep -q    ${ vmuuid } $ \nif [ $? -ne 0 ]; then\n        echo  error: no vm uuid \\ ${ vmuuid } \\  found \n        exit 2\nfi  The script then checks the power state of the VM and if it is running, it attempts a clean shutdown. The event system is used to wait for the VM to enter state \"Halted\".   Note \nThe XE CLI supports a command-line argument --minimal which causes it to print its output without excess whitespace or formatting, ideal for use from scripts. If multiple values are returned they are comma-separated.   # Check the power state of the vm\nname=$( ${ XE }  vm-list uuid= ${ vmuuid }  params=name-label --minimal)\nstate=$( ${ XE }  vm-list uuid= ${ vmuuid }  params=power-state --minimal)\nwasrunning=0\n\n# If the VM state is running, we shutdown the vm first\nif [  ${ state }  =  running  ]; then\n         ${ XE }  vm-shutdown uuid= ${ vmuuid } \n         ${ XE }  event-wait class=vm power-state=halted uuid= ${ vmuuid } \n        wasrunning=1\nfi  The VM is then cloned and the new VM has its name_label set to cloned_vm.  # Clone the VM\nnewuuid=$( ${ XE }  vm-clone uuid= ${ vmuuid }  new-name-label=cloned_vm)  Finally, if the original VM had been running and was shutdown, both it and the new VM are started.  # If the VM state was running before cloning, we start it again\n# along with the new VM.\nif [  $wasrunning  -eq 1 ]; then\n         ${ XE }  vm-start uuid= ${ vmuuid } \n         ${ XE }  vm-start uuid= ${ newuuid } \nfi", 
            "title": "4.3.2. Cloning a VM using the XE CLI"
        }, 
        {
            "location": "/#chapter-5-using-http-to-interact-with-xenserver", 
            "text": "XenServer exposes an HTTP interface on each host, that can be used to perform various operations. This chapter describes the available mechanisms.", 
            "title": "Chapter 5. Using HTTP to interact with XenServer"
        }, 
        {
            "location": "/#51-vm-import-and-export", 
            "text": "Because the import and export of VMs can take some time to complete, an asynchronous HTTP interface to the import and export operations is provided. To perform an export using the XenServer API, construct an HTTP GET call providing a valid session ID, task ID and VM UUID, as shown in the following pseudo code:  task = Task.create()\nresult = HTTP.get(\n  server, 80,  /export?session_id= session_id task_id= task_id ref= vm_uuid );  For the import operation, use an HTTP PUT call as demonstrated in the following pseudo code:  task =  Task . create ()  result =  HTTP . put ( \n  server ,   80 ,   /import?session_id= session_id task_id= task_id ref= vm_uuid );", 
            "title": "5.1. VM Import and Export"
        }, 
        {
            "location": "/#52-getting-xenserver-performance-statistics", 
            "text": "XenServer records statistics about the performance of various aspects of your XenServer installation. The metrics are stored persistently for long term access and analysis of historical trends. Where storage is available to a VM, the statistics are written to disk when a VM is shut down. Statistics are stored in RRDs (Round Robin Databases), which are maintained for individual VMs (including the control domain) and the server. RRDs are resident on the server on which the VM is running, or the pool master when the VM is not running. The RRDs are also backed up every day.   Warning \nIn earlier versions of the XenServer API, instantaneous performance metrics could be obtained using the VM_metrics, VM_guest_metrics, host_metrics methods and associated methods. These methods has been deprecated in favor of using the http handler described in this chapter to download the statistics from the RRDs on the VMs and servers. Note that by default the legacy metrics will return zeroes. To revert to periodic statistical polling as present in earlier versions of XenServer, set the other-config:rrd_update_interval=  parameters on your host to one of the following values, and restart your host:  never\nThis is the default, meaning no periodic polling is performed.  1 Polling is performed every 5 seconds.  2 Polling is performed every minute.  By default, the older metrics APIs will not return any values, and so this key must be enabled to run monitoring clients which use the legacy monitoring protocol.   Statistics are persisted for a maximum of one year, and are stored at different granularities. The average and most recent values are stored at intervals of:    5 seconds for the past 10 minutes    one minute for the past 2 hours    one hour for the past week    one day for the past year    RRDs are saved to disk as uncompressed XML. The size of each RRD when written to disk ranges from 200KiB to approximately 1.2MiB when the RRD stores the full year of statistics.   Warning \nIf statistics cannot be written to disk, for example when a disk is full, statistics will be lost and the last saved version of the RRD will be used.   Statistics can be downloaded over HTTP in XML format, for example using wget. See http://oss.oetiker.ch/rrdtool/doc/rrddump.en.html and http://oss.oetiker.ch/rrdtool/doc/rrdxport.en.html for information about the XML format. HTTP authentication can take the form of a username and password or a session token. Parameters are appended to the URL following a question mark (?) and separated by ampersands ( ).  To obtain an update of all VM statistics on a host, the URL would be of the form:  http:// username : password @ host /rrd_updates?start= secondssinceepoch \nThis request returns data in an rrdtool xport style XML format, for every VM resident on the particular host that is being queried. To differentiate which column in the export is associated with which VM, the legend field is prefixed with the UUID of the VM.  To obtain host updates too, use the query parameter host=true:  http:// username : password @ host /rrd_updates?start= secondssinceepoch host=true \nThe step will decrease as the period decreases, which means that if you request statistics for a shorter time period you will get more detailed statistics.  Additional rrd_updates parameters  cf= ave|min|max    the data consolidation mode interval= interval  the interval between values to be reported   Note \nBy default only ave statistics are available. To obtain min and max statistics for a VM, run the following command: xe pool-param-set uuid= pool_uuid  other-config:create_min_max_in_new_VM_RRDs \nTo obtain all statistics for a host:   http:// username:password@host /host_rrd \nTo obtain all statistics for a VM:  http:// username:password@host /vm_rrd?uuid= vm_uuid", 
            "title": "5.2. Getting XenServer Performance Statistics"
        }, 
        {
            "location": "/#chapter-6-xenserver-api-extensions", 
            "text": "The XenAPI is a general and comprehensive interface to managing the life-cycles of Virtual Machines, and offers a lot of flexibility in the way that XenAPI providers may implement specific functionality (e.g. storage provisioning, or console handling). XenServer has several extensions which provide useful functionality used in our own XenCenter interface. The workings of these mechanisms are described in this chapter.  Extensions to the XenAPI are often provided by specifying other-config map keys to various objects. The use of this parameter indicates that the functionality is supported for that particular release of XenServer, but not as a long-term feature. We are constantly evaluating promoting functionality into the API, but this requires the nature of the interface to be well-understood. Developer feedback as to how you are using some of these extensions is always welcome to help us make these decisions.", 
            "title": "Chapter 6. XenServer API extensions"
        }, 
        {
            "location": "/#61-vm-console-forwarding", 
            "text": "Most XenAPI graphical interfaces will want to gain access to the VM consoles, in order to render them to the user as if they were physical machines. There are several types of consoles available, depending on the type of guest or if the physical host console is being accessed:  Console access     Operating System  Text  Graphical  Optimized graphical      Windows  No  VNC, using an API call  RDP, directly from guest    Linux  Yes, through VNC and an API call  No  VNC, directly from guest    Physical Host  Yes, through VNC and an API call  No  No     Hardware-assisted VMs, such as Windows, directly provide a graphical console over VNC. There is no text-based console, and guest networking is not necessary to use the graphical console. Once guest networking has been established, it is more efficient to setup Remote Desktop Access and use an RDP client to connect directly (this must be done outside of the XenAPI).  Paravirtual VMs, such as Linux guests, provide a native text console directly. XenServer provides a utility (called vncterm) to convert this text-based console into a graphical VNC representation. Guest networking is not necessary for this console to function. As with Windows above, Linux distributions often configure VNC within the guest, and directly connect to it over a guest network interface.  The physical host console is only available as a vt100 console, which is exposed through the XenAPI as a VNC console by using vncterm in the control domain.  RFB (Remote Framebuffer) is the protocol which underlies VNC, specified in The RFB Protocol. Third-party developers are expected to provide their own VNC viewers, and many freely available implementations can be adapted for this purpose. RFB 3.3 is the minimum version which viewers must support.", 
            "title": "6.1. VM console forwarding"
        }, 
        {
            "location": "/#611-retrieving-vnc-consoles-using-the-api", 
            "text": "VNC consoles are retrieved using a special URL passed through to the host agent. The sequence of API calls is as follows:    Client to Master/443: XML-RPC: Session.login_with_password().    Master/443 to Client: Returns a session reference to be used with subsequent calls.    Client to Master/443: XML-RPC: VM.get_by_name_label().    Master/443 to Client: Returns a reference to a particular VM (or the \"control domain\" if you want to retrieve the physical host console).    Client to Master/443: XML-RPC: VM.get_consoles().    Master/443 to Client: Returns a list of console objects associated with the VM.    Client to Master/443: XML-RPC: VM.get_location().    Returns a URI describing where the requested console is located. The URIs are of the form: https://192.168.0.1/console?ref=OpaqueRef:c038533a-af99-a0ff-9095-c1159f2dc6a0.    Client to 192.168.0.1: HTTP CONNECT \"/console?ref=(...)\"    The final HTTP CONNECT is slightly non-standard since the HTTP/1.1 RFC specifies that it should only be a host and a port, rather than a URL. Once the HTTP connect is complete, the connection can subsequently directly be used as a VNC server without any further HTTP protocol action.  This scheme requires direct access from the client to the control domain's IP, and will not work correctly if there are Network Address Translation (NAT) devices blocking such connectivity. You can use the CLI to retrieve the console URI from the client and perform a connectivity check.  To retrieve a console URI using the CLI    Retrieve the VM UUID by running:  xe vm-list params=uuid --minimal name-label=name    Retrieve the console information:    xe console-list vm-uuid=uuid\nuuid ( RO): 714f388b-31ed-67cb-617b-0276e35155ef\nvm-uuid ( RO): 8acb7723-a5f0-5fc5-cd53-9f1e3a7d3069\nvm-name-label ( RO): etch\nprotocol ( RO): RFB\nlocation ( RO): https://192.168.0.1/console?ref=(...)\nUse command-line utilities like ping to test connectivity to the IP address provided in the location field.", 
            "title": "6.1.1. Retrieving VNC consoles using the API"
        }, 
        {
            "location": "/#612-disabling-vnc-forwarding-for-linux-vm", 
            "text": "When creating and destroying Linux VMs, the host agent automatically manages the vncterm processes which convert the text console into VNC. Advanced users who wish to directly access the text console can disable VNC forwarding for that VM. The text console can then only be accessed directly from the control domain directly, and graphical interfaces such as XenCenter will not be able to render a console for that VM.  Disabling a Linux VNC console using the CLI    Before starting the guest, set the following parameter on the VM record:  xe vm-param-set uuid=uuid other-config:disable_pv_vnc=1 \n2. Start the VM.    Use the CLI to retrieve the underlying domain ID of the VM with:\n    `xe vm-list params=dom-id uuid=  --minimal   On the host console, connect to the text console directly by: /usr/lib/xen/bin/xenconsole  `\nThis configuration is an advanced procedure, and we do not recommend that the text console is directly used for heavy I/O operations. Instead, connect to the guest over SSH or some other network-based connection mechanism.", 
            "title": "6.1.2. Disabling VNC forwarding for Linux VM"
        }, 
        {
            "location": "/#62-paravirtual-linux-installation", 
            "text": "The installation of paravirtual Linux guests is complicated by the fact that a Xen-aware kernel must be booted, rather than simply installing the guest using hardware-assistance. This does have the benefit of providing near-native installation speed due to the lack of emulation overhead. XenServer supports the installation of several different Linux distributions, and abstracts this process as much as possible.  To this end, a special bootloader known as eliloader is present in the control domain which reads various other-config keys in the VM record at start time and performs distribution-specific installation behavior.    install-repository - Required. Path to a repository; 'http', 'https', 'ftp', or 'nfs'. Should be specified as would be used by the target installer, but not including prefixes, e.g. method=.    install-vnc - Default: false. Use VNC where available during the installation.    install-vncpasswd - Default: empty. The VNC password to use, when providing one is possible using the command-line of the target distribution.    install-round - Default: 1. The current bootloader round. Not to be edited by the user (see below)", 
            "title": "6.2. Paravirtual Linux installation"
        }, 
        {
            "location": "/#621-red-hat-enterprise-linux-4144", 
            "text": "eliloader is used for two rounds of booting. In the first round, it returns the installer initrd and kernel from /opt/xensource/packages/files/guest-installer. Then, on the second boot, it removes the additional updates disk from the VM, switches the bootloader to pygrub, and then begins a normal boot.  This sequence is required since Red Hat does not provide a Xen kernel for these distributions, and so the XenServer custom kernels for those distributions are used instead.", 
            "title": "6.2.1. Red Hat Enterprise Linux 4.1/4.4"
        }, 
        {
            "location": "/#622-red-hat-enterprise-linux-4550", 
            "text": "Similar to the RHEL4.4 installation, except that the kernel and ramdisk are downloaded directly form the network repository that was specified by the user, and switch the bootloader to pygrub immediately. Note that pygrub is not executed immediately, and so will only be parsed on the next boot.  The network retrieval enables users to install the upstream Red Hat vendor kernel directly from their network repository. An updated XenServer kernel is also provided on the xs-tools.iso built-in ISO image which fixes various Xen-related bugs.", 
            "title": "6.2.2. Red Hat Enterprise Linux 4.5/5.0"
        }, 
        {
            "location": "/#623-suse-enterprise-linux-10-sp1", 
            "text": "This requires a two-round boot process. The first round downloads the kernel and ramdisk from the network repository and boots them. The second round then inspects the disks to find the installed kernel and ramdisk, and sets the PV-bootloader-args to reflect these paths within the guest filesystem. This process emulates the domUloader which SUSE use as an alternative to pygrub. Finally, the bootloader is set to pygrub and is executed to begin a normal boot.  The SLES 10 installation method means that the path for the kernel and ramdisk is stored in the VM record rather than in the guest menu.lst, but this is the only way it would ever work since the YAST package manager doesn't write a valid menu.lst.", 
            "title": "6.2.3. SUSE Enterprise Linux 10 SP1"
        }, 
        {
            "location": "/#624-centos-45-50", 
            "text": "The CentOS installation mechanism is similar to that of the Red Hat installation notes above, save that some MD5 checksums are different which eliloader recognizes.", 
            "title": "6.2.4. CentOS 4.5 / 5.0"
        }, 
        {
            "location": "/#63-adding-xenstore-entries-to-vms", 
            "text": "Developers may wish to install guest agents into VMs which take special action based on the type of the VM. In order to communicate this information into the guest, a special Xenstore name-space known as vm-data is available which is populated at VM creation time. It is populated from the xenstore-data map in the VM record.  To populate a Xenstore node foo in a VM    Set the xenstore-data parameter in the VM record:  xe vm-param-set uuid= vm_uuid  xenstore-data:vm-data/foo=bar\u00a1\u00a4    Start the VM.    If it is a Linux-based VM, install the XenServer Tools and use the xenstore-read to verify that the node exists in Xenstore.     Note \nOnly prefixes beginning with vm-data are permitted, and anything not in this name-space will be silently ignored when starting the VM.", 
            "title": "6.3. Adding Xenstore entries to VMs"
        }, 
        {
            "location": "/#64-security-enhancements", 
            "text": "The control domain in XenServer 7.0 and above has various security enhancements in order to harden it against attack from malicious guests. Developers should never notice any loss of correct functionality as a result of these changes, but they are documented here as variations of behavior from other distributions.    The socket interface, xenstored, access using libxenstore. Interfaces are restricted by xs_restrict().    The device /dev/xen/evtchn, which is accessed by calling xs_evtchn_open() in libxenctrl. A handle can be restricted using xs_evtchn_restrict().    The device /proc/xen/privcmd, accessed through xs_interface_open() in libxenctrl. A handle is restricted using xc_interface_restrict(). Some privileged commands are naturally hard to restrict (e.g. the ability to make arbitrary hypercalls), and these are simply prohibited on restricted handles.    A restricted handle cannot later be granted more privilege, and so the interface must be closed and re-opened. Security is only gained if the process cannot subsequently open more handles.    The control domain privileged user-space interfaces can now be restricted to only work for certain domains. There are three interfaces affected by this change:    The qemu device emulation processes and vncterm terminal emulation processes run as a non-root user ID and are restricted into an empty directory. They uses the restriction API above to drop privileges where possible.    Access to xenstore is rate-limited to prevent malicious guests from causing a denial of service on the control domain. This is implemented as a token bucket with a restricted fill-rate, where most operations take one token and opening a transaction takes 20. The limits are set high enough that they should never be hit when running even a large number of concurrent guests under loaded operation.    The VNC guest consoles are bound only to the localhost interface, so that they are not exposed externally even if the control domain packet filter is disabled by user intervention.", 
            "title": "6.4. Security enhancements"
        }, 
        {
            "location": "/#65-advanced-settings-for-network-interfaces", 
            "text": "Virtual and physical network interfaces have some advanced settings that can be configured using the other-config map parameter. There is a set of custom ethtool settings and some miscellaneous settings.", 
            "title": "6.5. Advanced settings for network interfaces"
        }, 
        {
            "location": "/#651-ethtool-settings", 
            "text": "Developers might wish to configure custom ethtool settings for physical and virtual network interfaces. This is accomplished with ethtool-  keys in the other-config map parameter.     Key  Description  Valid settings      ethtool-rx  Specify if RX checksumming is enabled  on or true to enable the setting, off or false to disable it    ethtool-tx  Specify if TX checksumming is enabled  on or true to enable the setting, off or false to disable it    ethtool-sg  Specify if scatter-gather is enabled  on or true to enable the setting, off or false to disable it    ethtool-tso  Specify if tcp segmentation offload is enabled  on or true to enable the setting, off or false to disable it    ethtool-ufo  Specify if UDP fragmentation offload is enabled  on or true to enable the setting, off or false to disable it    ethtool-gso  Specify if generic segmentation offload is enabled  on or true to enable the setting, off or false to disable it    ethtool-autoneg  Specify if autonegotiation is enabled  on or true to enable the setting, off or false to disable it    ethtool-speed  Set the device speed in Mb/s  10, 100, or 1000    ethtool-duplex  Set full or half duplex mode  half or full     For example, to enable TX checksumming on a virtual NIC using the xe CLI: xe vif-param-set uuid= VIF UUID  other-config:ethtool-tx=\"on\"  or:  xe vif-param-set uuid= VIF UUID  other-config:ethtool-tx=\"true\" \nTo set the duplex setting on a physical NIC to half duplex using the xe CLI:  xe vif-param-set uuid= VIF UUID  other-config:ethtool-duplex=\"half\"", 
            "title": "6.5.1. ethtool settings"
        }, 
        {
            "location": "/#652-miscellaneous-settings", 
            "text": "You can also set a promiscuous mode on a VIF or PIF by setting the promiscuous key to on. For example, to enable promiscuous mode on a physical NIC using the xe CLI:  xe pif-param-set uuid= PIF UUID  other-config:promiscuous=\"on\" \nor:  xe pif-param-set uuid= PIF UUID  other-config:promiscuous=\"true\" \nThe VIF and PIF objects have a MTU parameter that is read-only and provide the current setting of the maximum transmission unit for the interface. You can override the default maximum transmission unit of a physical or virtual NIC with the mtu key in the other-config map parameter. For example, to reset the MTU on a virtual NIC to use jumbo frames using the xe CLI:  xe vif-param-set uuid= VIF UUID  other-config:mtu=9000 \nNote that changing the MTU of underlying interfaces is an advanced and experimental feature, and may lead to unexpected side-effects if you have varying MTUs across NICs in a single resource pool.", 
            "title": "6.5.2. Miscellaneous settings"
        }, 
        {
            "location": "/#66-internationalization-for-sr-names", 
            "text": "The SRs created at install time now have an other_config key indicating how their names may be internationalized.  other_config[\"i18n-key\"] may be one of    local-hotplug-cd    local-hotplug-disk    local-storage    xenserver-tools    Additionally, other_config[\"i18n-original-value- \"] gives the value of that field when the SR was created. If XenCenter sees a record where SR.name_label equals other_config[\"i18n-original-value-name_label\"] (that is, the record has not changed since it was created during XenServer installation), then internationalization will be applied. In other words, XenCenter will disregard the current contents of that field, and instead use a value appropriate to the user's own language.  If you change SR.name_label for your own purpose, then it no longer is the same as other_config[\"i18n-original-value-name_label\"]. Therefore, XenCenter does not apply internationalization, and instead preserves your given name.", 
            "title": "6.6. Internationalization for SR names"
        }, 
        {
            "location": "/#67-hiding-objects-from-xencenter", 
            "text": "Networks, PIFs, and VMs can be hidden from XenCenter by adding the key HideFromXenCenter=true to the other_config parameter for the object. This capability is intended for ISVs who know what they are doing, not general use by everyday users. For example, you might want to hide certain VMs because they are cloned VMs that shouldn't be used directly by general users in your environment.  In XenCenter, hidden Networks, PIFs, and VMs can be made visible, using the  View  menu.", 
            "title": "6.7. Hiding objects from XenCenter"
        }, 
        {
            "location": "/#chapter-7-xencenter-api-extensions", 
            "text": "The following section details the assumptions and API extensions that we have made, over and above the documented API. Extensions are encoded as particular key-value pairs in dictionaries such as VM.other_config.", 
            "title": "Chapter 7. XenCenter API Extensions"
        }, 
        {
            "location": "/#71-pool", 
            "text": "Key  Semantics      pool.name_label  An empty name_label indicates that the pool should be hidden on the tree view.    pool.rolling_upgrade_in_progress  Present if the pool is in the middle of a rolling upgrade.", 
            "title": "7.1. Pool"
        }, 
        {
            "location": "/#72-host", 
            "text": "Key  Semantics      host.other_config[\"iscsi_iqn\"]  The host's iSCSI IQN.    host.license_params[\"expiry\"]  The expiry date of the host's license, in ISO 8601, UTC.    host.license_params[\"sku_type\"]  The host license type i.e. Server or Enterprise.    host.license_params[\"restrict_pooling\"]  Returns true if pooling is restricted by the host.    host.license_params[\"restrict_connection\"]  The number of connections that can be made from XenCenter is restricted.    host.license_params[\"restrict_qos\"]  Returns true if Quality of Service settings are enabled on the host.    host.license_params[\"restrict_vlan\"]  Returns true if creation of virtual networks is restricted on the host.    host.license_params[\"restrict_pool_attached_storage\"]  Returns true if the creation of shared storage is restricted on this host.    host.software_version[\"product_version\"]  Returns the host's product version.    host.software_version[\"build_number\"]  Returns the host's build number.    host.software_version[\"xapi\"]  Returns the host's api revision number.    host.software_version[\"package-linux\"]  Returns \"installed\" if the Linux pack has been installed.    host.software_version[\"oem_build_number\"]  If the host is the OEM version, return its revision number.    host.logging[\"syslog_destination\"]  Gets or sets the destination for the XenServer system logger (null for local logging).    host.logging[\"multipathing\"]  \"true\" if storage multipathing is enabled on this host.    host.logging[\"boot_time\"]  A floating point Unix time giving the time that the host booted.    host.logging[\"agent_start_time\"]  A floating point Unix time giving the time that the control domain management daemon started.", 
            "title": "7.2. Host"
        }, 
        {
            "location": "/#73-vm", 
            "text": "Key  Semantics      VM.other_config[\"default_template\"]  This template is one that was installed by Citrix. This is used to selectively hide these in the tree view, to use a different icon for them, and to disallow deletion.    VM.other_config[\"xensource_internal\"]  This template is special, such as the P2V server template. These are completely hidden by the UI.    VM.other_config[\"install_distro\"] == \"rhlike\"  This template is for RHEL 4.5, RHEL 5, or CentOS equivalents. This is used to prompt for the Install Repository during install, including support for install from ISO / CD on Miami, and to modify NFS URLs to suit these installers.    VM.other_config[\"install_distro\"] in { \"rhel41\"  \"rhel44\" }    VM.other_config[\"install_distro\"] == \"sleslike\"  This template is for SLES 10 and SLES 9. This is used to prompt for the Install Repository during install, like the EL5 ones, but in this case the NFS URLs are not modified. ISO support is available for SLES 10 on XenServer 7.0. Use install-methods to distinguish between SLES 9 and SLES 10 on that platform.    VM.other_config[\"install-repository\"] == \"cdrom\"  Requests an install from a repository in the VM's attached CD drive, rather than a URL.    VM.other_config[\"auto_poweron\"]  Gets or sets whether the VM starts when the server boots, \"true\" or \"false\".    VM.other_config[\"ignore_excessive_vcpus\"]  Gets or sets to ignore XenCenter's warning if a VM has more VCPUs than its host has physical CPUs, true to ignore.    VM.other_config[\"HideFromXenCenter\"]  Gets or sets whether XenCenter will show the VM in the treeview, \"true\" to hide.    VM.other_config[\"import_task\"]  Gets the import task that created this VM.    VM.HVM_boot_params[\"order\"]  Gets or sets the VM's boot order on HVM VM's only, e.g. \"CDN\" will boot in the following order - First boot disk, CD drive, Network.    VM.VCPU_params[\"weight\"]  Gets or sets the IONice value for the VM's VCPUs, ranges from 1 to 65536, 65536 being the highest.    VM.pool_migrate(..., options['live'])  true indicates live migration. XenCenter always uses this.    VM.other_config[\"install-methods\"]  A comma-separated list of install methods available for this template. May include \"cdrom\", \"nfs\", \"http\" or \"ftp\".    VM.other_config[\"last_shutdown_time\"]  The time that this VM was last shut down or rebooted, formatted as a UTC ISO8601 datetime.    VM.other_config[\"p2v_source_machine\"]  The source machine, if this VM was imported by a P2V process.    VM.other_config[\"p2v_import_date\"]  The date the VM was imported, if it was imported by a P2V process. Formatted as a UTC ISO8601 datetime.", 
            "title": "7.3. VM"
        }, 
        {
            "location": "/#74-sr", 
            "text": "Key  Semantics      SR.other_config[\"auto-scan\"]  The SR will be automatically scanned for changes. Set on all SRs created by XenCenter.    SR.sm_config[\"type\"]  Set as type cd for SRs which are physical CD drives.", 
            "title": "7.4.  SR"
        }, 
        {
            "location": "/#75-vdi", 
            "text": "Key  Semantics      VDI.type  user instead of system is used to mean \"do or do not allow deletion of the VDI through the GUI, if this disk is attached to a VM\". The intention here is to prevent you from corrupting a VM (you should uninstall it instead). suspend and crashdump record suspend and core dumps respectively. ephemeral is currently unused.    VDI.managed  All unmanaged VDIs are completely hidden in the UI. These are branch points in VHD chains, or unused LUN-per-VDI disks.    VDI.sm_config[\"vmhint\"]  The UUID of the VM that this VDI supports. This is set when VDIs are created through the user interface, to improve performance for certain storage backends.", 
            "title": "7.5. VDI"
        }, 
        {
            "location": "/#76-vbd", 
            "text": "Key  Semantics      VBD.other_config[\"is_owner\"]  If set, then this disk may be deleted when the VM is uninstalled.    VBD.other_config[\"class\"]  Set to an integer, corresponding to the Best Effort setting of ionice.", 
            "title": "7.6. VBD"
        }, 
        {
            "location": "/#77-network", 
            "text": "Key  Semantics      network.other_config[\"automatic\"]  The New VM wizard will create a VIF connected to this network by default, if this key has any value other than false.    network.other_config[\"import_task\"]  Gets the import task that created this network.", 
            "title": "7.7. Network"
        }, 
        {
            "location": "/#78-vm_guest_metrics", 
            "text": "Key  Semantics      PV_drivers_version[\"major\"]  Gets the major version of the VM's PV drivers' version.    PV_drivers_version[\"minor\"]  Gets the minor version of the VM's PV drivers' version.    PV_drivers_version[\"micro\"]  Gets the micro (build number) of the VM's PV drivers' version.", 
            "title": "7.8. VM_guest_metrics"
        }, 
        {
            "location": "/#79-task", 
            "text": "Key  Semantics      task.other_config[\"object_creation\"] == \"complete\"  For the task associated with a VM import, this flag will be set when all the objects (VMs, networks) have been created. This is useful in the import VM wizard for us to then go and re-map all the networks that need it.", 
            "title": "7.9. Task"
        }
    ]
}